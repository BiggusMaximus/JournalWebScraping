title,link,number_of_citation,article_type,publisher,publication_date,abstract,keyword
ADAPTER: A DRL-Based Approach to Tune Routing in WSNs,https://ieeexplore.ieee.org/document/10225792/,1,Conference Paper,IEEE,2023,"As an essential part of the Internet of Things, Wireless Sensor Networks (WSNs) require high energy efficiency and Quality of Service (QoS). Most routing algorithms used in WSNs suffer from two significant limitations: the inability to accommodate multiple optimization objectives and to quickly adjust to dynamically changing environments. In order to address these limitations, we propose an adaptive selection approach of routing algorithm based on Deep Reinforcement Learning, called ADAPTER. This approach allows the system to intelligently select among various routing algorithms in response to changes in the system state, resulting in improved dual optimization of energy efficiency and QoS metrics. Our experiments demonstrate that ADAPTER outperforms a single routing algorithm in terms of network lifetime, average end-to-end delay, and packet loss rate.","Wireless Sensor Networks ,  System State ,  Service Quality ,  Energy Efficiency ,  Internet Of Things ,  Multi-objective Optimization ,  Multiple Objects ,  Deep Reinforcement Learning ,  Adaptive Selection ,  Single Algorithm ,  Routing Algorithm ,  Efficiency Metrics ,  Network Lifetime ,  Single Route ,  Packet Loss Rate ,  Response Algorithm ,  Environmental Changes ,  Energy Consumption ,  Performance Metrics ,  Network Operators ,  Deep Reinforcement Learning Agent ,  Energy Of Nodes ,  Sink Node ,  Residual Energy ,  Critic Network ,  Proximal Policy Optimization ,  Markov Decision Process ,  Relay Nodes ,  Reward Function ,  Data Packets "
ELISE: A Reinforcement Learning Framework to Optimize the Slotframe Size of the TSCH Protocol in IoT Networks,https://ieeexplore.ieee.org/document/10473707/,0,Journal Article,IEEE,2024,"The Internet of Things is shaping the next generation of cyberâ€“physical systems to improve the future industry for smart cities. It has created novel and essential applications that require specific network performance to enhance the quality of services. Since network performance requirements are application-oriented, it is of paramount importance to provide tailored solutions that seamlessly manage the network resources and orchestrate the network to satisfy user requirements. In this article, we propose ELISE, a reinforcement learning (RL) framework to optimize the slotframe size of the time slotted channel hopping protocol in IIoT networks while considering the user requirements. We primarily address the problem of designing a framework that self-adapts to the optimal slotframe length that best suits the user's requirements. The framework takes care of all functionalities involved in the correct functioning of the network, while the RL agent instructs the framework with a set of actions to determine the optimal slotframe size each time the user requirements change. We evaluate the performance of ELISE through extensive analysis based on simulations and experimental evaluations on a testbed to demonstrate the efficiency of the proposed approach in adapting network resources at runtime to satisfy user requirements.","Reinforcement Learning Framework ,  IoT Networks ,  Time Slotted Channel Hopping ,  Internet Of Things ,  Network Performance ,  Experimental Evaluation ,  Time Slot ,  Network Resources ,  User Requirements ,  Reinforcement Learning Agent ,  Performance Metrics ,  Power Consumption ,  Power Network ,  Dependability ,  Reward Function ,  Markov Decision Process ,  Wireless Sensor Networks ,  Average Delay ,  Changes In Requirements ,  Network Reliability ,  Control Plane ,  Average Power Consumption ,  Proximal Policy Optimization ,  Received Signal Strength Indicator ,  Network Delay ,  Deep Q-network ,  Control Packets ,  Average Reliability ,  Packet Header ,  Routing Algorithm "
PEARL: Power and Delay-Aware Learning-based Routing Policy for IoT Applications,https://ieeexplore.ieee.org/document/9849862/,0,Conference Paper,IEEE,2022,"Routing between the IoT nodes has been considered an important challenge, due to its impact on different link/node metrics, including power consumption, reliability, and latency. Due to the low-power and lossy nature of IoT environments, the amount of consumed power, and the ratio of delivered packets plays an important role in the overall performance of the system. Meanwhile, in some IoT applications, e.g., remote health-care monitoring systems, other factors such as End-to-End (E2E) latency is significantly crucial. The standardized routing mechanism for IoT networks (RPL) tries to optimize these parameters via specified routing policies in its Objective Function (OF). The original version of this protocol, and many of its existing extensions are not well-suited for dynamic IoT networks. In the past few years, reinforcement learning methods have significantly involved in dynamic systems, where agents have no acknowledgment about their surrounding environment. These techniques provide a predictive model based on the interaction between an agent and its environment to reach a semi-optimized solution; For instance, the matter of packet transmission, and their delivery in unstable IoT networks. Accordingly, this paper introduces PEARL; a machine-learning based routing policy for IoT networks, which is both, delay-aware, and power-efficient. PEARL employs a novel routing policy based on the q-learning algorithm, which uses the one-hop E2E delay as its main path selection metric to determine the rewards of the algorithm, and to improve the E2E delay, and consumed power simultaneously in terms of Power-Delay-Product (PDP). According to an extensive set of experiments conducted in the Cooja simulator, in addition to improving reliability in the network in terms of Packet Delivery Ratio (PDR), PEARL has improved the amount of E2E delay, and PDP metrics in the network by up to 61% and 72%, against the state-of-the-art, respectively.","Internet Of Things ,  Internet Of Things Applications ,  Routing Policy ,  Objective Function ,  Power Consumption ,  Internet Of Things Networks ,  Q-learning Algorithm ,  Internet Of Things Nodes ,  Amount Of Delay ,  Packet Delivery Ratio ,  Routing Mechanism ,  Dense Network ,  Root Node ,  Neighboring Nodes ,  Optimal Path ,  Markov Decision Process ,  Wireless Sensor Networks ,  Data Packets ,  Reinforcement Learning Algorithm ,  Increase In Transmission ,  Candidate Parents ,  Received Signal Strength Indicator ,  Maximum Reward ,  Receiver Node ,  Static Network ,  Control Packets ,  Mobile Nodes ,  Path Delay ,  Data Packet Transmission ,  Control Messages "
Multi-agent reinforcement learning for intelligent resource allocation in IIoT networks,https://ieeexplore.ieee.org/document/9692913/,4,Conference Paper,IEEE,2021,"In the industrial Internet of Things (IIoT), a high number of devices with limited resources, like computational power, memory, bandwidth and, in case of wireless sensor networks, also energy, communicate. At the same time, the amount of data as well as the demand for data processing in the edge is rapidly increasing. To enable Industry 4.0 (I4.0) and the IIoT, an intelligent resource allocation is required to make optimal use of the available resources. For this purpose, a multi-agent system (MAS) based on deep reinforcement learning (DRL) is proposed. Multi-agent reinforcement learning (MARL) is already taken into account in different communication networks, e.g. for intelligent routing. Despite its great potential, little attention is paid to these methods in industry so far. In this work, DRL is applied for resource allocation and load balancing for industrial edge computing. An optimal usage of the available resources of the IIoT devices should be achieved. Due to the structure of IIoT systems as well as for security reasons, a MAS is preferred for decentralized decision making. In subsequent steps, it is planned to add and remove devices during runtime, to change the number of tasks to be executed as well as evaluations on single- and multi-policy-approaches. The following aspects will be considered for evaluation: (1) improvement of the resource usage of the devices and (2) overhead due to the MAS.","Intelligence ,  Resource Allocation ,  Industrial Internet Of Things ,  Multi-agent Reinforcement Learning ,  Limited Resources ,  Communication Network ,  Internet Of Things ,  Multi-agent Systems ,  Deep Reinforcement Learning ,  Edge Computing ,  Wireless Sensor Networks ,  Load Balancing ,  Industrial Methods ,  Optimal Use Of Resources ,  Resources Of Devices ,  Local Environment ,  State Space ,  Resource Consumption ,  Data Streams ,  Edge Devices ,  Data Processing Algorithms ,  Task Offloading ,  Decentralized System ,  Communication Load "
Reinforcement learning-based algorithm for efficient and adaptive forwarding in named data networking,https://ieeexplore.ieee.org/document/8330354/,11,Conference Paper,IEEE,2017,"In order to solve the severe issues of current TCP/IP Internet architecture, Named Data Networking(NDN) recently attracts lots of researchers, deemed to be one of the most futuristic Internet paradigms among numerous Information-Centric Networking(ICN) proposals. Although the forwarding strategy is the key feature of NDN, it is still at a preliminary stage. Current forwarding strategies either base on the flooding strategy trying to reduce the side effect of Interest flooding or base on the route-driven strategy paying attention to decrease the extra cost in maintaining the routing information. They are both deficient because of the storm issues or too much extra overhead. In this paper, we present the feasibility of using reinforcement learning algorithm such as Q-Learning in NDN. By modifying Q-Learning algorithm to solve the inherent issues, we design and implement IQ-Learning(Interest Q-Learning) strategy and DQ-Learning(Data Q-Learning) strategy, which learn from the past experience and make the best forwarding choice. The simulation results on NDNSim show that our forwarding strategies achieve higher Interest satisfaction ratio, shorter Interest satisfaction delay and are more sensitive to the changes of network than Flooding strategy and the state-of-the-art BestRoute strategy.","Named Data Networking ,  Simulation Results ,  Network Changes ,  Reinforcement Learning Algorithm ,  Forward Scheme ,  Routing Information ,  Extra Overhead ,  Q-learning Algorithm ,  Network Topology ,  Central Node ,  Sensor Networks ,  Updated Information ,  Neighboring Nodes ,  Markov Decision Process ,  High Adaptability ,  Wireless Sensor Networks ,  Data Packets ,  Node Information ,  Forward Process ,  Low Delay ,  Packet Forwarding ,  Reinforcement Learning Techniques "
An Intelligent Fault Tolerant Data Routing Scheme for Wireless Sensor Network-Assisted Industrial Internet of Things,https://ieeexplore.ieee.org/document/9878177/,10,Journal Article,IEEE,2023,"Safety is a major concern for Industrial 4.0 where different physical parameters are monitored for avoiding uncertain events in the industry. In industries, natural calamities like fire and leakage of harmful gases can cause huge damage to life and property. An Industrial Internet of Things (IIoT) is used to monitor such natural calamities and take timely prompt actions. However, sensors in the IIoT are vulnerable to failures due to energy depletion and hardware malfunctioning. It significantly reduces the reliability of the network. This article proposes an intelligent fault-tolerant scheme where different faults within the wireless sensor network-assisted IIoT such as node fault and link fault are detected and tolerated in a timely manner. It significantly improves the reliability of the network. Extensive simulations show the out-performance of the proposed scheme in terms of average packet delivery, energy consumption, throughput, network lifetime, communication delay, and recovery speed.","Internet Of Things ,  Fault-tolerant ,  Wireless Sensor ,  Industrial Internet Of Things ,  Intelligent Data ,  Energy Consumption ,  Average Energy ,  Average Lifetime ,  Average Delay ,  Communication Delay ,  Average Recovery ,  Network Reliability ,  Average Network ,  Average Energy Consumption ,  Network Packets ,  Network Lifetime ,  Delivery Speed ,  Consumption Speed ,  Average Delivery ,  Link Failure ,  Reinforcement Learning Algorithm ,  Node Failure ,  Sink Node ,  Wireless Sensor Networks ,  Residual Energy ,  Gas Sensors ,  Cluster Head ,  Temperature Sensor "
Reinforcement Learning and Vector-based Clustering Method to Achieve Energy Balanced in Underwater Acoustic Sensor Networks,https://ieeexplore.ieee.org/document/10608266/,0,Conference Paper,IEEE,2024,"Underwater acoustic sensor networks (UASNs) have a wide range of applications, such as water quality monitoring, ocean data collection and underwater resource exploration. In UASN applications, the management of energy is very important. Sensor nodes in UASNs are usually powered by batteries, which are very difficult to replace and replenish. Extending the life cycle by reducing the energy consumption of UASNs can reduce the network operation cost. Therefore, the energy management problem has become a popular research topic in UASNs. The efficiency of cluster routing in reducing the communication overhead between nodes in the network has been verified by dividing nodes into clusters and having the cluster header (CH) responsible for communicating with the base station and other nodes. However, if the cluster headers and paths are not properly chosen, the energy consumption of the nodes will be unbalanced, thus leading to premature death of the nodes. In this paper, a reinforcement learning and vector-based clustering (RVC) method is proposed to achieve energy balancing in UASNs. We use reinforcement learning and vector forwarding methods to select appropriate CHs and relay nodes to balance the node energy consumption based on the residual energy of the nodes, the transmission distance and the node density in the direction of the routing vector. The simulation results show that the method can effectively reduce the energy consumption of UASN nodes, achieve energy balancing, and extend the network survival cycle.","Clustering Method ,  Reinforcement Learning Methods ,  Acoustic Networks ,  Energy Consumption ,  Wide Range Of Applications ,  Unit Vector ,  Base Station ,  Transmission Distance ,  Residual Energy ,  Relay Nodes ,  Cluster Head ,  Energy Of Nodes ,  Density In Direction ,  Learning Algorithms ,  Local Information ,  Data Transmission ,  Network Performance ,  Transmission Routes ,  Average Energy ,  Acoustic Waves ,  Sink Node ,  Direct Transmission ,  Cluster Nodes ,  Node Depth ,  Reward Function ,  Energy Consumption Model ,  Target Node ,  Energy Utilization Efficiency ,  Node Failure ,  Connecting Lines "
CAPL: Criticality-Aware Adaptive Path Learning for Industrial Wireless Sensorâ€“Actuator Networks,https://ieeexplore.ieee.org/document/9968202/,2,Journal Article,IEEE,2023,"Wireless technologies, such as WirelessHART, are being adopted in industrial wireless sensorâ€“actuator networks (IWSAN), which are required to provide reliable quality of control (QoC). This article focuses on adaptively selecting the best network path for reliable QoC in the IWSAN. The main challenge is estimating the time-varying packet delivery ratio (PDR) of each path. The IWSAN path selection problem in a multi-armed bandit (MAB) framework is formulated. A novel algorithm criticality-aware adaptive path learning (CAPL) is proposed, which determines the criticality of each packet according to the degree of QoC degradation if it is lost. The key novelty of CAPL is that it simultaneously considers the fundamental explorationâ€“exploitation trade-off in MAB and QoC in the IWSAN. CAPL uses low-criticality packets for exploration to measure the PDR so that it can minimize the impact of exploration on QoC degradation. CAPL with extensive simulation and empirical studies for DC motor position control is validated.","Wireless Networks ,  Industrial Networks ,  Industrial Wireless ,  Quality Control ,  DC Motor ,  Path Selection ,  Multi-armed Bandit ,  Packet Delivery Ratio ,  Control System ,  Actuator ,  Actual Values ,  Control Design ,  Sensor Data ,  Physical System ,  Control Performance ,  Time Slot ,  Settling Time ,  Optimal Path ,  Wireless Sensor Networks ,  Packet Loss ,  Control Packets ,  Upper Confidence Bound ,  Routing Path ,  Critical Concept ,  Bandit Problem ,  Medium Access Control ,  Wireless Environment ,  Number Of Rewards ,  Queue Size ,  Efficient Learning "
Reinforcement Learning Based Stochastic Shortest Path Finding in Wireless Sensor Networks,https://ieeexplore.ieee.org/document/8886484/,30,Journal Article,IEEE,2019,sion Critical Sensors and Sensor Networks (MC-SSN),"Shortest Path ,  Sensor Networks ,  Wireless Sensor Networks ,  Stochastic Path ,  Stochastic Shortest Path ,  Random Variables ,  Edge Length ,  Markov Decision Process ,  Reinforcement Learning Algorithm ,  Stochastic Problem ,  Practical Algorithm ,  Network Load ,  Average Reward ,  Competitive Algorithm ,  Shortest Path Problem ,  Value Function ,  Convergence Rate ,  Faster Convergence ,  Path Planning ,  Correct Path ,  Optimal Path ,  Greedy Policy ,  Q-function ,  Reward Function ,  State-action Pair ,  Total Path ,  Proof Of Convergence ,  Reinforcement Learning Agent ,  State-value Function "
Reinforcement learning-based best path to best gateway scheme for wireless mesh networks,https://ieeexplore.ieee.org/document/6085373/,12,Conference Paper,IEEE,2011,"This paper addresses the problem of optimal routing in backbone wireless mesh networks (WMNs) where each mesh router (MR) is equipped with multiple radio interfaces and a subset of nodes serve as gateways to the Internet. Most routing schemes have been designed to reduce routing costs by optimizing one metric, e.g., hop count, load at routers and interference. However, when considering these metrics together, the complexity of the routing problem increases drastically. Thus, an efficient and adaptive routing scheme that takes into account several metrics simultaneously is needed. In this paper, we propose an efficient new routing scheme, called RLBPR (Reinforcement Learning-based Best Path Routing), that adaptively learns an optimal routing policy, depending on multiple optimization metrics such as loss ratio, interference ratio and load at the gateways. Simulation results show that RLBPR can significantly improve the overall network performance compared to schemes using either Metric of interference and channel switching (MIC), Best Path to Best Gateway (BP2BG), Expected Transmission count (ETX), nearest gateway (i.e., shortest path to gateway) or load at gateways as a metric for path selection.","Wireless Networks ,  Mesh Network ,  Wireless Mesh ,  Wireless Mesh Networks ,  Shortest Path ,  Optimal Policy ,  Pathfinding ,  Loss Ratio ,  Routing Scheme ,  Routing Path ,  Interference Ratio ,  Optimal Metrics ,  Hop Count ,  Signal-to-noise ,  Throughput ,  Data Rate ,  Learning Rate ,  Types Of Networks ,  Quality Metrics ,  Bit Error Rate ,  Source Node ,  Packet Loss Rate ,  Packet Loss ,  Delivery Ratio ,  Congestion Level ,  Wireless Sensor Networks ,  Different Types Of Networks ,  Co-channel Interference ,  Ad Hoc Networks ,  Signal-to-interference-plus-noise Ratio "
Co-CEStat: Cooperative Critical Data Transmission in Emergency in Static Wireless Body Area Network,https://ieeexplore.ieee.org/document/7016057/,9,Conference Paper,IEEE,2014,"Cooperative routing is a promising technique which exploits the broadcast nature of wireless medium to enhance network performance. Sensor nodes simultaneously transmit their data on different links and utilize cooperation between nodes. In this paper, a new protocol, Co-CEStat, Cooperative Critical data transmission in Emergency for Static Wireless Body Area Networks, is proposed. The protocol utilizes the merits of both direct and cooperative transmission to achieve higher stability period and end-to-end throughput with greater network lifetime. Simulations are conducted in MATLAB to compare the Co-CEStat's performance with that of three other contemporary non-cooperative routing protocols.","Data Transmission ,  Wireless Networks ,  Body Area Networks ,  Wireless Body Area Networks ,  Stable Period ,  Direct Transmission ,  Network Lifetime ,  Greater Lifetime ,  Energy Consumption ,  Information Transmission ,  Energy Transmission ,  Central Body ,  Wireless Sensor Networks ,  Types Of Nodes ,  Source Node ,  Network Throughput ,  Residual Energy ,  Path Loss Exponent ,  Network Energy ,  Routing Algorithm ,  Normal Nodes ,  Multi-hop Communication ,  Energy Of Nodes ,  Reliable Delivery ,  Received Signal Strength Indicator ,  Dynamic Routing ,  Cooperative Communication ,  Multi-agent Reinforcement Learning ,  Network Flow ,  Heterogeneous Network "
"Reinforcement Learning based Security Policy to Mitigate Wormhole, Blackhole and Grayhole Attacks in MANET",https://ieeexplore.ieee.org/document/10486553/,0,Conference Paper,IEEE,2024,"This research study applies reinforcement learning to the problem of preventing wormhole, blackhole and grayhole attacks on mobile ad hoc networks (MANETs). The flexible actor-critic architecture is utilized here to implement reinforcement learning. Utilization of wireless sensor networks in the development of a variety of communication infrastructures is increasing. Multiple types of intrusions can be launched against wireless network sensors. Due to these sensor devices, a wireless network is susceptible to a denial of service attack. One is a wormhole, blackhole and grayhole attack, in which two malicious sensor nodes are connected via a low-latency link to disrupt the normal routing of the network. Researchers are increasingly attracted to machine learning techniques due to their ability to uncover previously unknown hazards. Our primary objective is to identify these intricate patterns and construct a secure mobile ad hoc network by prioritizing security measures such as detecting and blocking malicious nodes. Simulation results demonstrate the efficient mitigation of wormhole, blackhole and grayhole attack.","Black Hole ,  Ad Hoc Networks ,  Wormhole ,  Machine Learning ,  Wireless Networks ,  Wireless Sensor Networks ,  Denial Of Service ,  Malicious Nodes ,  Learning Algorithms ,  Training Dataset ,  Convolutional Neural Network ,  Support Vector Machine ,  Decision Tree ,  Assault ,  State Space ,  K-nearest Neighbor ,  Mobile Network ,  Anomaly Detection ,  Reward Function ,  Markov Decision Process ,  Intrusion Detection System ,  Deep Q-network ,  Intrusion Detection ,  Mobile Nodes ,  Reinforcement Learning Algorithm ,  Huber Loss ,  Target Node "
Optimal Routing Protocol in LPWAN Using SWC: A Novel Reinforcement Learning Framework,https://ieeexplore.ieee.org/document/10478282/,0,Journal Article,IEEE,2024,"Low-power wide-area network (LPWAN) has emerged as a dominating communication technology that offers low-power and wide coverage for the Internet of Things (IoT) applications. However, the direct data transmission approach has a limited network lifetime. Even multihop data transmission experiences many difficulties including high data latency, poor bandwidth utilization, and reduced data throughput. To overcome these challenges, in this article, a recent breakthrough in social networks known as small-world characteristics (SWC) is incorporated into LPWANs. In particular, in this work, small-world LPWANs (SW-LPWANs) are developed by using the reinforcement learning (RL) technique and using different node centrality measures like degree, betweenness, and closeness centrality. Furthermore, the performance of the developed SW-LPWANs is evaluated in terms of energy efficiency (alive/dead devices, and network residual energy) and quality-of-service (QoS) (average data latency, data throughput, and bandwidth utilization) and is compared with that of conventional multihop LPWAN. Finally, to validate the simulation results, similar analyses are performed on the real-field LPWAN testbed. The obtained simulation results confirm that an SW-LPWAN developed by the RL method performs better than other techniques, with 
 ${11}\%$ 
 more alive devices, 
 ${5}.{5}\%$ 
 higher residual energy, 
 ${2}.{4}\%$ 
 improved data throughput, and 
 ${14}\%$ 
 efficient bandwidth utilization compared to the next best method. A similar trend is observed with real-field LPWAN testbed data also.","Pathfinding ,  Low Power Wide Area Networks ,  Small-world Characteristics ,  Social Networks ,  Simulation Results ,  Learning Techniques ,  Energy Efficiency ,  Internet Of Things ,  Data Transmission ,  Utilization Efficiency ,  Centrality Measures ,  Betweenness Centrality ,  Degree Centrality ,  Internet Of Things Applications ,  Average Latency ,  Residual Energy ,  Reinforcement Learning Methods ,  Data Throughput ,  Reinforcement Learning Techniques ,  Wide Area Network ,  Average Path Length ,  Conventional Network ,  Wireless Sensor Networks ,  Betweenness Centrality Measures ,  High Clustering Coefficient ,  Wireless Local Area Network ,  Internet Of Things Networks ,  Wireless Networks ,  Shortest Path ,  Data Packets "
Congestion Aware Q-Learning (CAQL) in RPL Protocol â€“ WSN based IoT Networks,https://ieeexplore.ieee.org/document/9888322/,6,Conference Paper,IEEE,2022,"In an Internet of Things (IoT) based real network scenario, the miscellaneous sensor applications are present which produce a heterogeneous traffic prototype. RPL (Routing Protocol for Low-power and Lossy Networks) is an attractive model for effective routing techniques in a heterogeneous traffic. The major objective functions (OFs) used in this prototype are objective function zero (OF0) and the Minimum Rank with Hysteresis Objective Function (MRHOF). But the routing pattern of standard OFs are not suitable for such network because it creates more congestion which leads to link failure and retransmission during communication. In our research, we observed the RPL protocol from base for heterogeneous traffic and proposed a novel routing protocol with a Reinforcement Learning Approach namely Congestion Aware Q-Learning (CAQL) in RPL Routing Protocol. CAQL technique is helpful for maintaining queue in optimal manner and load balancing between the nodes in the network to prevent the congestion, which lead to increase the performance of network in terms of Quality of Service (QoS). The implementation of the proposed model is done using the Network Simulator (NS2). The simulation of CAQL-RPL is done and the results are compared with the earlier protocol such as MRHOM, QWL-RPL. The performance evaluation shows that CAQL-RPL can provide better performance in a heterogeneous traffic prototype in terms of packet delivery ratio (PSR) and Packet loss ratio (PLR), Throughput, Energy Consumption and Network Delay.","Internet Of Things ,  Internet Of Things Networks ,  Objective Function ,  Service Quality ,  Network Performance ,  Simulated Networks ,  Load Balancing ,  Reinforcement Learning Approach ,  Early Protocol ,  Packet Delivery Ratio ,  Minimum Rank ,  Learning Algorithms ,  Support Vector Machine ,  Value Function ,  Supervised Learning ,  Independent Component Analysis ,  Root Node ,  Early Methods ,  Markov Decision Process ,  Directed Acyclic Graph ,  Selection Of Parents ,  Ranking Of Factors ,  Multi-armed Bandit ,  Feedback Function ,  Network Throughput ,  Child Nodes ,  Node Level ,  Control Messages ,  Reward Signal ,  Network Congestion "
Learning-based routing approach for direct interactions between wireless sensor network and moving vehicles,https://ieeexplore.ieee.org/document/6728518/,3,Conference Paper,IEEE,2013,"Wireless sensor network (WSN) has been applied to traffic information collection in many researches. Moving vehicles in the road need to acquire real time traffic information directly from WSN to make proper decisions and avoid potential accidents. Due to the large scale deployment of WSN, routing is necessary to deliver real time information to vehicles in a multi-hop way. Taking the moving vehicle as a mobile sink, a reinforcement learning-based routing approach is proposed to support sink mobility and enable direct interactions between WSN and vehicles. Multiple metrics including time delay, network lifetime and reliability are ensured by designing a comprehensive reward function for learning. The convergence speed of learning is improved with sink announcement. Simulation results show the feasibility of the proposed approach for direct interactions between WSN and moving vehicles. Comparisons are also carried out to show the superiority of the proposed approach over other approaches.","Direct Interaction ,  Sensor Networks ,  Learning-based Approaches ,  Wireless Sensor Networks ,  Vehicle Motion ,  Routing Approach ,  Time Delay ,  Collection Of Information ,  Reward Function ,  Real Information ,  Traffic Information ,  Network Lifetime ,  Data Transmission ,  Network Performance ,  Energy Distribution ,  Optimal Policy ,  Delivery Rate ,  Traffic Conditions ,  Data Packets ,  Road Conditions ,  Hop Count ,  Vehicular Ad Hoc Networks ,  Road Vehicles ,  Residual Energy ,  Reward Information ,  Routing Table ,  Vehicular Applications ,  Received Signal Strength ,  Routing Algorithm ,  Optimal Metrics "
Mobile Charger Utility Maximization through Preemptive Scheduling for Rechargeable WSNs,https://ieeexplore.ieee.org/document/9719458/,3,Conference Paper,IEEE,2021,"Prolong the network lifetime is the primary challenge of wireless sensor networks (WSNs). There are several ways to enhance the lifetime, such as through efficient routing, mobile sink, energy harvesting, etc. A recent breakthrough on wireless charging introduced another state-of-art technique to prolong the network lifetime by recharging the sensor nodes wirelessly. However, selecting the efficient order to charge the nodes among the requesting nodes is a challenging task. In this context, we propose a mobile charger utility maximization approach through preemptive scheduling (MCUMPS) for wireless rechargeable sensor networks (WRSNs). In this approach, we mainly focus on the visiting order of the mobile charger by segregating the sensor nodes into two categories called critical nodes and emerging nodes. Critical nodes (CNs) are high priority than the emerging nodes because the dying of CNs isolate some part of the network. So, we tested the proposed MCUMPS algorithm through simulation, and we observe the surmount performance over the existing but related works.","Wireless Sensor Networks ,  Charge Mobility ,  Utility Maximization ,  Preemptive Scheduling ,  Wireless Rechargeable Sensor Networks ,  Challenging Task ,  Energy Harvesting ,  Sensor Networks ,  Wireless Sensor ,  Critical Nodes ,  Wireless Charging ,  Network Lifetime ,  Estimation Algorithm ,  Time Of Occurrence ,  Leaf Node ,  Wireless Power Transfer ,  Deep Reinforcement Learning ,  Charging Time ,  Scheduling Scheme ,  Scheduling Approach ,  Priority Queue ,  Network Partitioning "
Autonomic and Distributed Joint Routing and Power Control for Delay-Sensitive Applications in Multi-Hop Wireless Networks,https://ieeexplore.ieee.org/document/5648764/,53,Journal Article,IEEE,2011,"Multi-hop wireless networks can provide flexible network infrastructures at a low cost. However, most existing wireless networking solutions are designed for delay-insensitive applications, thereby resulting in poor performance when handling delay-sensitive applications. Traditionally, network design problems are formulated as static optimizations, by assuming the network characteristics remain static. However, these solutions are not optimal when the environments are dynamic. Recently, several research works apply machine learning to maximize the performance of multi-hop wireless networks in dynamic environments, but they either only focus on determining policies at the network layer, without considering the lower-layers' actions, or use centralized learning approaches, which are inefficient for delay-sensitive applications, due to the large delay when propagating messages throughout the network. We propose in this paper a new solution that enables the nodes to autonomously determine their routing and transmission power to maximize the network utility, in a dynamic environment. We formulate the problem as a Markov Decision Process, and propose a distributed computation of the optimal policy. Moreover, we use reinforcement-learning to find the optimized policy when the dynamics are unknown. We explicitly consider the impact of the information overhead on the network performance, and propose several novel algorithms to reduce the information overhead.","Wireless Networks ,  Power Control ,  Joint Power ,  Delay-sensitive Applications ,  Joint Power Control ,  Multi-hop Wireless Networks ,  Joint Routing ,  Dynamic Environment ,  Network Layer ,  Network Performance ,  Transmission Power ,  Optimal Policy ,  Distributed Computing ,  Markov Decision Process ,  Learning Center ,  Large Delay ,  Network Utility ,  Multi-hop Networks ,  Network Design Problem ,  Value Function ,  State Transition Probability ,  Time Slot ,  Routing Decisions ,  Value Function Approximation ,  Independent Markov Chain ,  Queue Size ,  Data Packets ,  Information Exchange ,  Frequency Of Feedback ,  Relay Selection "
Performance Enhancement in Underwater Optical Wireless Communication using Deep Reinforcement Learning Approach,https://ieeexplore.ieee.org/document/10290678/,0,Conference Paper,IEEE,2023,"Recent years have witnessed increased studies focusing on underwater optical wireless communication, also known as UOWC. This is due to the proliferation of new uses for this technology, including military defense, ecological control, and marine ecological preservation. A developing method for direct transmission over the water surface is UOWC. Nevertheless, because of restricted energy supplies and an architecture that is relatively variable due to water flow motion, it is challenging to offer low-consumption and dependable routing in UOWC because of the high directionality of optical rays and the harsh marine conditions. It is necessary to provide enhanced communication for UOWC. As a result, a deep reinforcement learning (DRL) strategy is suggested to improve underwater communication. To better respond to a changing ecosystem and extend system longevity, the network is initially represented as a deep decentralized network, with residual power and connection reliability considered while designing the routing mechanism. The effectiveness of the proposed system is assessed by comparison with existing systems. Several assessment standards, including packet delivery ratio, error rate, energy consumption, and computation time. The findings from the simulation demonstrate that the proposed system can deliver dependable communication reliably and effectively.","Deep Learning ,  Deep Reinforcement Learning ,  Optical Communication ,  Deep Reinforcement Learning Approach ,  Underwater Wireless Optical Communication ,  Energy Consumption ,  Error Rate ,  Computation Time ,  Bit Error Rate ,  Underwater Communication ,  Packet Error ,  Packet Delivery Ratio ,  Routing Mechanism ,  Light-emitting Diodes ,  Local System ,  Digital Communication ,  Sensor Networks ,  Data Exchange ,  Bit Error ,  Wireless Sensor Networks ,  Adjacent Nodes ,  Silicon Photomultiplier ,  Routing Algorithm ,  Routing Method ,  Information Packets ,  Undersea ,  Orthogonal Frequency Division Multiplexing ,  Incentive System ,  Communication Range ,  Sink Node "
Emergency Navigation in Confined Spaces Using Dynamic Grouping,https://ieeexplore.ieee.org/document/7373229/,2,Conference Paper,IEEE,2015,"The performance of Emergency Management Systems (EMS) in confined spaces is highly dependent on the decision algorithm employed for the safe navigation of the evacuees to the available exits. In the algorithm proposed in this paper, we have considered evacuees under two groups, based on their age and physical condition, and we tailor two routing metrics, one for each group, in finding suitable paths for the evacuees. A dynamic grouping mechanism that can adjust an evacuee's group, and therefore routing metric, according to its on-going health condition is employed during the evacuation. To implement the routing metrics, we have used the Cognitive Packet Network (CPN) with random neural networks (RNN) and reinforcement learning. The CPN is an adaptive routing protocol that is loop-free at all times and easily handles multiple quality of service (QoS) metrics. Simulation results show that allowing the navigation system to be sensitive to the on-going health conditions and mobility of the evacuees, using our proposed dynamic grouping, can achieve higher survival rates.","Survival Rate ,  Physical Conditions ,  Service Quality ,  Dynamic Mechanism ,  Cognitive Networks ,  Network Packets ,  Random Neural Network ,  Learning Algorithms ,  Spatial Information ,  Model Building ,  Information And Communication Technologies ,  Shortest Path ,  Individual Classes ,  Levels Of Fatigue ,  Wireless Sensor Networks ,  Arrival Rate ,  Dijkstraâ€™s Algorithm ,  Routing Algorithm ,  Time Metrics ,  Path Information ,  Use Of Spatial Information ,  Hazard Intensity ,  Evacuation Time ,  Emergency Evacuation "
Quality of Service Factor based Unfailing Route Formation in Wireless Sensor Network,https://ieeexplore.ieee.org/document/10090966/,36,Conference Paper,IEEE,2023,"Nowadays, several analytical methods exist to make unfailing route formation. However, it is still challenging work because the wireless medium is unreliable. Sensor nodes are utilized for widespread distribution. Thus, security issues greatly affect wireless sensor networks (WSN). The Quality of Service Factor-based Unfailing Route (QFUR) Formation in WSN is introduced to solve these issues. In this approach, Quality of Service (QoS) factors like sensor node delay, packet drop, and residual energy is verified to determine whether the sensor node is normal. The value of the QoS factor is less than the threshold for that node to be a normal sensor node in the WSN. This approach is serious to notice and separate the compromised nodes from evading misinformation through the falsified data inserted by the opponent over compromised nodes. The Reinforcement Learning (RL) algorithm computes the reward value based on node energy utilization, hop count, dropped rate, and delay. The simulation outcomes demonstrate a lesser delay and lesser packet loss in the network. Furthermore, it improves the opponent node's detection and minimizes the false negative ratio in the WSN.","Service Quality ,  Wireless Sensor Networks ,  Energy Utilization ,  Reinforcement Learning Algorithm ,  Network Loss ,  Residual Energy ,  Network Delay ,  Normal Nodes ,  Packet Drop ,  Hop Count ,  Communication Network ,  Energy Availability ,  Value Of Node ,  Drop Rate ,  Simulated Networks ,  Technical Results ,  Received Signal Strength ,  Routing Algorithm ,  Authentication Method ,  Incentive Value "
A Multi-AUV Collaborative Ocean Data Collection Method Based on LG-DQN and Data Value,https://ieeexplore.ieee.org/document/10272719/,3,Journal Article,IEEE,2024,"As a result of the development of the Internet of Underwater Things (IoUT), underwater connected devices generate a large volume of data with varying values and time sensitivity. Previous data collection strategies cannot accommodate the varying time requirements of various data types. To address the aforementioned issues, this article proposes a cooperative data collection method (MADC-DV) for multiple autonomous underwater vehicles (AUVs) based on local global deep 
 $Q$ 
 learning (LG-DQN) and data value, which divides data into emergency and nonemergency and achieves hybrid data collection. First, the MAC protocol for communication between AUVs and clusters is designed to divide nonemergency data into high-value data and low-value data, with low-value data not needing to reply to ACK acknowledgment packets, thereby reducing the nonemergency data collection delay. Second, nonemergency data are collected cooperatively using multiple AUVs, and the LG-DQN approach is used to plan the paths for multiple AUV data collection in order to reduce the overall energy consumption of underwater wireless sensor networks (UWSNs). Finally, emergency data are collected using a multihop routing approach to assist in the collection. A routing method is proposed to compensate for the inability of AUVs to be applied to emergency data collection. The experimental results indicate that the method can improve the network life cycle by 18.7%, reduce the delay in the collection of nonemergency data by 40%, and reduce the delay in the collection of emergency data by 26.3%, thereby meeting the varying time requirements for different types of data.","Data Collection Methods ,  Ocean Data ,  Collaborative Method ,  Energy Consumption ,  Deep Learning ,  Sensor Networks ,  Time Requirements ,  Communication Protocol ,  Wireless Sensor ,  Wireless Sensor Networks ,  Autonomous Underwater Vehicles ,  Multiple Data Collection ,  Routing Method ,  Network Energy Consumption ,  Data Collection In Order ,  Average Energy ,  Network Size ,  Area Data ,  Path Planning ,  Neighboring Nodes ,  Cluster Head ,  Routing Algorithm ,  Cluster Nodes ,  Data Packets ,  Value Threshold ,  Transmission Phase ,  Hold Time ,  Energy Of Nodes ,  Delay Data ,  Deep Reinforcement Learning "
Managing Crowds in Hazards With Dynamic Grouping,https://ieeexplore.ieee.org/document/7152824/,38,Journal Article,IEEE,2015,"Emergency navigation algorithms for evacuees in confined spaces typically treat all evacuees in a homogeneous manner, using a common metric to select the best exit paths. In this paper, we present a quality of service (QoS) driven routing algorithm to cater to the needs of different types of evacuees based on age, mobility, and level of resistance to fatigue and hazard. Spatial information regarding the location and the spread of hazards is also integrated into the routing metrics to avoid situations where evacuees may be directed toward hazardous zones. Furthermore, rather than persisting with a single decision algorithm during an entire evacuation process, we suggest that evacuees may adapt their course of action with regard to their ongoing physical condition and environment. A widely tested routing protocol known as the cognitive packet network with random neural networks and reinforcement learning are employed to collect information and provide advice to evacuees, and is beneficial in emergency navigation owing to its low computational complexity and its ability to handle multiple QoS metrics in its search for safe exit paths. The simulation results indicate that the proposed algorithm, which is sensitive to the needs of evacuees, produces better results than the use of a single metric. Simulations also show that the use of dynamic grouping to adjust the evacuees' category, and routing algorithms that have regard for their on-going health conditions and mobility, can achieve higher survival rates.","Survival Rate ,  Service Quality ,  Spatial Information ,  Course Of Action ,  Cognitive Networks ,  Decision Algorithm ,  Ongoing State ,  Routing Algorithm ,  Path Search ,  Network Packets ,  Random Neural Network ,  Safe Path ,  Performance Of Algorithm ,  Disaster Management ,  Information And Communication Technologies ,  Walking Speed ,  Emergency Situations ,  Occupational Level ,  Wireless Sensor Networks ,  Arrival Rate ,  Use Of Spatial Information ,  Linear Programming Algorithm ,  Floor Of Building ,  Time Metrics ,  Evacuation Time ,  Path Information ,  Hazard Intensity ,  Network Routing ,  Emergency Evacuation ,  Decision Node "
Q-learning enhanced gradient based routing for balancing energy consumption in WSNs,https://ieeexplore.ieee.org/document/7790324/,6,Conference Paper,IEEE,2016,"Energy is a sparse and valuable resource in Wireless Sensor Networks (WSNs). Using it efficiently and effectively can mean longer node and network lifetimes with less reliance and strain on energy harvesting components. In this work we focus on optimizing energy consumption in WSNs at the network layer. A gradient based routing protocol is proposed, that integrates a Reinforcement Learning (RL) component to learn and seek out routes which deplete node energy in a balanced manner. This RL enhanced protocol was compared against three other gradient based protocols, including a greedy, shortest-paths variation serving as a baseline. All protocols were simulated under conditions of heavily imbalanced network load. Based on the simulation results, the network lifetime of the RL enhanced protocol was nearly doubled in comparison to the baseline protocol, while average packet delay was significantly reduced.","Wireless Sensor Networks ,  Balance Energy Consumption ,  Network Layer ,  Shortest Path ,  Energy Harvesting ,  Sensor Networks ,  Network Load ,  Network Lifetime ,  Energy Of Nodes ,  Energy Levels ,  Base Station ,  Neighboring Nodes ,  Reward Function ,  Physical Layer ,  Data Frame ,  Final Destination ,  Battery Capacity ,  Considerable Energy ,  Residual Energy ,  Application Layer ,  Simulation Run Time ,  Transmission Medium "
Intrusion Prevention Scheme Against Rank Attacks for Software-Defined Low Power IoT Networks,https://ieeexplore.ieee.org/document/9978298/,2,Journal Article,IEEE,2022,"The 6LoWPAN (IPv6 over low-power wireless personal area networks) standard enables resource-constrained devices to connect to the IPv6 network, blending an IPv6 header compression protocol. For this network technology, a new routing protocol called Routing Protocol for Low Power Lossy network (RPL) has been designed. The latter is a lightweight protocol that determines the route across the nodes based on rank values. This protocol is known to be non-resilient against Rank attacks, which aim at creating non-optimized routes for packet forwarding, hence overwhelming the constrained 6LoWPAN. With 5G, Software-Defined Networks (SDNs) have been developed to facilitate simple programmable control plane, Quality of Service (QoS) provisioning, and route configuration services for 6LoWPAN. However, there is still a lack of optimization mechanisms to protect 6LoWPAN against Rank attacks in SDN-based deployment. To this end, in this paper, a Reinforcement-Learning (RL) agent is leveraged to assist and complement an SDN controller in achieving cost-efficient route optimization, and QoS provisioning packet forwarding to prevent rank attacks. Experimental results confirm that our approach effectively prevents Rank attacks while providing an adequate delay and radio duty cycle. Meanwhile, it maximizes the packet delivery ratio, facilitating practical implementations in software-defined Low Power Internet of Things (IoT) networks.","Low Power ,  Internet Of Things ,  Internet Of Things Networks ,  Intrusion Prevention System ,  Service Quality ,  Duty Cycle ,  Pathfinding ,  Rank Value ,  Control Plane ,  Packet Delivery ,  Packet Forwarding ,  Packet Delivery Ratio ,  Network Topology ,  Root Node ,  Application Programming Interface ,  Internet Of Things Devices ,  Reward Function ,  Optimal Path ,  Directed Acyclic Graph ,  Wireless Sensor Networks ,  Internet Of Things Nodes ,  Reinforcement Learning Agent ,  Flow Table ,  Topology Optimization ,  Quality Of Service Requirements ,  Control Messages ,  Reinforcement Learning Approach ,  Routing Path ,  Routing Table ,  Malicious Nodes "
Deep Reinforcement Learning-Based Approach for Efficient and Reliable Droplet Routing on MEDA Biochips,https://ieeexplore.ieee.org/document/9844153/,2,Journal Article,IEEE,2023,"The micro-electrode-dot-array (MEDA) architecture provides precise droplet control and real-time sensing in digital microfluidic biochips. Previous work has shown that trapped charge under microelectrodes (MCs) leads to droplets being stuck and failures in fluidic operations. A recent approach utilizes real-time sensing of MC health status, and attempts to avoid degraded electrodes during droplet routing. However, the problem with this solution is that the computational complexity is unacceptable for MEDA biochips of realistic size. Consequently, in this work, we introduce a deep reinforcement learning (DRL)-based approach to bypass degraded electrodes and enhance the reliability of routing. The DRL model utilizes the information of health sensing in real time to proactively reduce the likelihood of charge trapping and avoid using degraded MCs. Simulation results show that our approach provides effective routing strategies for COVID-19 testing protocols. We also validate our DRL-based approach using fabricated prototype biochips. Experimental results show that the developed DRL model completed the routing tasks using a fewer number of clock cycles and shorter total execution time, compared with a baseline routing method. Moreover, our DRL-based approach provides reliable routing strategies even in the presence of degraded electrodes. Our experimental results show that the proposed DRL-based routing is robust to occurrences of electrode faults, as well as increases the lifetime and usability of microfluidic biochips compared to existing strategies.","Electrode ,  Deep Reinforcement Learning ,  Charge Trapping ,  Routing Scheme ,  Deep Reinforcement Learning Model ,  Convolutional Neural Network ,  Deep Neural Network ,  Actuator ,  Number Of Steps ,  State Space ,  Droplet Size ,  Traditional Learning ,  Performance Gain ,  Reward Function ,  Training Environment ,  Convolutional Neural Network Training ,  Deep Neural Network Architecture ,  Control Board ,  Routing Model ,  Degradation Parameters ,  Deep Reinforcement Learning Agent ,  Proximal Policy Optimization ,  Number Of Actuators ,  Deep Reinforcement Learning Framework ,  Camera Module ,  Successful Execution ,  Transfer Learning ,  Electrode Array ,  Droplet Movement ,  Bioassay "
