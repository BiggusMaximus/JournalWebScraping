title,link,number_of_citation,article_type,publisher,publication_date,abstract,keyword
Emerging Trends of ML-based Intelligent Services for Industrial Internet of Things (IIoT),https://ieeexplore.ieee.org/document/9018815/,11,Conference Paper,IEEE,2019,"Intelligent information technology is a notable feature in the context of industry 4.0. A key factor in obtaining intelligent industrial Internet of things (IIoT) services is to integrate machine learning (ML) into IIoT. With the increasing scale of deployed terminals, IIoT becomes heterogeneous, diverse, and dynamically changeable. Traditional optimization methods are difficult to deal with the emerging network problems. This paper first proposes a ML-based IIoT architecture for intelligent IIoT services and expounds two ML methods for IIoT analysis, namely, deep learning (DL) and reinforcement learning (RL). Secondly, advanced applications and development trends of ML in industrial field are summarized. Opportunities and challenges of ML for IIoT analysis are discussed finally. The purpose of this paper is to point out the role of artificial intelligence (AI) technology in IIoT from the macroscopic view.","Internet Of Things ,  Intelligence Services ,  Industrial Internet Of Things ,  Industrial Internet ,  Machine Learning ,  Deep Learning ,  Machine Learning Methods ,  Artificial Intelligence Technology ,  Network Growth ,  Problem In Networks ,  Integration Of Machine Learning ,  Internet Of Things Services ,  Role Of Artificial Intelligence ,  Traditional Optimization Methods ,  Machine Learning Challenges ,  Learning Algorithms ,  Machine Learning Models ,  Deep Learning Models ,  Wireless Networks ,  Generative Adversarial Networks ,  Smart Grid ,  Deep Reinforcement Learning ,  Smart Manufacturing ,  Operation And Maintenance ,  Wireless Sensor Networks ,  Wide Area Network ,  Transport Layer ,  Information Security ,  Knowledge Discovery ,  Intelligent Transportation "
A Survey of Machine Learning Techniques Applied to Software Defined Networking (SDN): Research Issues and Challenges,https://ieeexplore.ieee.org/document/8444669/,455,Journal Article,IEEE,2019,"In recent years, with the rapid development of current Internet and mobile communication technologies, the infrastructure, devices and resources in networking systems are becoming more complex and heterogeneous. In order to efficiently organize, manage, maintain and optimize networking systems, more intelligence needs to be deployed. However, due to the inherently distributed feature of traditional networks, machine learning techniques are hard to be applied and deployed to control and operate networks. Software defined networking (SDN) brings us new chances to provide intelligence inside the networks. The capabilities of SDN (e.g., logically centralized control, global view of the network, software-based traffic analysis, and dynamic updating of forwarding rules) make it easier to apply machine learning techniques. In this paper, we provide a comprehensive survey on the literature involving machine learning algorithms applied to SDN. First, the related works and background knowledge are introduced. Then, we present an overview of machine learning algorithms. In addition, we review how machine learning algorithms are applied in the realm of SDN, from the perspective of traffic classification, routing optimization, quality of service/quality of experience prediction, resource management and security. Finally, challenges and broader perspectives are discussed.","Machine Learning ,  Machine Learning Techniques ,  Intelligence ,  Learning Algorithms ,  Resource Management ,  Machine Learning Applications ,  Global View ,  Pathfinding ,  Deep Learning ,  Support Vector Machine ,  Random Forest ,  Deep Neural Network ,  Service Quality ,  Internet Of Things ,  Unmanned Aerial Vehicles ,  Traffic Flow ,  Quality Of Experience ,  Optimal Network ,  Network Resources ,  Self-organizing Map ,  Intrusion Detection System ,  Virtual Network Functions ,  Intrusion Detection ,  Supervised Learning Algorithms ,  Control Plane ,  Random Neural Network ,  Reinforcement Learning Algorithm ,  Labeled Training Dataset ,  Wireless Sensor Networks ,  Flow Table "
An Adaptive SVM-based Routing Protocol for Underwater Acoustic Sensor Networks,https://ieeexplore.ieee.org/document/10244430/,1,Conference Paper,IEEE,2023,"Due to the harsh environment and high deployment cost, deploying underwater acoustic sensor networks (UASNs) is a challenging problem. The underwater networking technology for UASNs is becoming a hot research topic in recent years, where designing an appropriate routing protocol is crucial to effectively resolve the routing void, the packet delivery delay, and the energy utilization. In this paper, we propose an adaptive support-vector-machine-based routing (ASVMR) protocol in UASNs to prolong the network lifetime and reduce the end-to-end delay. The proposed protocol employs a distributed routing approach that considers all types of node state information to optimize the routing path dynamically in real-time. Moreover, a ""routing vector"" (a vector from the source to the sink node) is established, and a reasonable pipe radius is selected based on the degree of node sparsity. In addition, future states of nodes are adopted for decision-making. Simulation results show that the proposed ASVMR protocol performs well in terms of the packet delivery ratio, the end-to-end delay, the energy efficiency in dynamic underwater network environments.","Adaptive Routing ,  Acoustic Networks ,  Future Conditions ,  Sink Node ,  Degree Of Sparsity ,  Support Vector Machine ,  Dense Network ,  Internet Of Things ,  Kernel Function ,  Support Vector Machine Model ,  Bit Error Rate ,  Radial Basis Function Kernel ,  Bit Error ,  Data Packets ,  Residual Energy ,  Undersea ,  Decision Value ,  Mobile Nodes ,  Node Distance ,  Multi-agent Reinforcement Learning ,  Node Deployment ,  Node Depth ,  Hop Count ,  Number Of Sensor Nodes ,  Binary Phase Shift Keying ,  Recovery Mechanisms ,  Packet Loss ,  Node Density ,  Medium Access Control ,  Additive Noise "
Multi-Agent Reinforcement Learning for Mobile Crowdsensing Systems with Dedicated Vehicles on Road Networks,https://ieeexplore.ieee.org/document/9564834/,6,Conference Paper,IEEE,2021,"Vehicle fleets with on-board sensors hold promise for cost-effective mobile crowdsensing in urban areas. How such a vehicle fleet navigate collectively through a road network is critical for ensuring sufficient spatial-temporal coverage of the sensors to meet domain-specific requirements. In this paper, we develop multi-agent reinforcement learning algorithms (MARL) for centralized vehicle routing on road networks to optimize the spatial-temporal coverage. We construct an environment that is capable of incorporating user-defined weightings in a space-time domain to be covered by mobile sensing. We train the routing policy in the environment with two RL algorithms: proximal policy optimization and deep Q network in a multi-agent setting. Numerical tests on two grid networks (of sizes 20 × 20 and 30 × 30) show the proposed MARL algorithms can improve the performance by at most 56% compared with a heuristic random routing policy. Furthermore, the sensitivity analysis against different fleet sizes implies that a small number of dedicated vehicles is able to approach the limit of coverage for squared road networks. The codes for numerical experiments can be accessed at https://github.com/SpartanBin/mobile_crowd_sensing.","Road Network ,  Multi-agent Reinforcement Learning ,  Mobile Crowdsensing ,  Mobile Crowdsensing Systems ,  Environmental Policy ,  Reinforcement Learning Algorithm ,  Vehicle Routing ,  Onboard Sensors ,  Vehicle Fleet ,  Proximal Policy Optimization ,  Time Step ,  Local Information ,  Convolutional Layers ,  Shortest Path ,  State Value ,  Spatial Coverage ,  Temporal Differences ,  Urban Regions ,  Reward Function ,  Gradient Descent Method ,  Deep Q-network ,  Policy Model ,  Position Of Agent ,  Current Time Step ,  Reinforcement Learning Policy ,  Exponential Linear Unit ,  Duration Of Episodes "
Energy Optimization and Trajectory Planning for Constrained Multi-UAV Data Collection in WSNs,https://ieeexplore.ieee.org/document/10388295/,1,Journal Article,IEEE,2024,"Wireless sensor networks (WSNs) deployed in remote areas face a challenge in uploading the collected data to data centers due to limited network coverage. Unmanned aerial vehicles (UAVs) can extend network coverage to remote WSNs by flying and communicating with WSN aggregator nodes to collect data. UAV-assisted data collection systems need to be carefully developed to collect all data efficiently while considering the UAV and WSN constraints. This paper provides an energy-efficient multi-UAV data collection framework for WSNs. We formulate the data collection system as a problem that jointly optimizes the system cost and energy consumption constrained by the communication power, UAV mission time, and memory size. The problem is resolved over two steps: First, the location and number of aggregators needed are determined using a triangulation-based K-means clustering that minimizes the number of aggregators used and the system cost. Second, the dockstation position that minimizes the energy consumption is obtained using the gaining-sharing knowledge (GSK) optimization algorithm. The optimum UAV trajectory for each GSK candidate solution is designed by solving a capacitated vehicle routing problem (CVRP) that combines heuristic and metaheuristic solving techniques. Simulations show that our framework outperforms other recent techniques by minimizing the overall system cost and energy consumption.","Optimal Energy ,  Wireless Sensor Networks ,  Trajectory Planning ,  Energy Consumption ,  Energy System ,  Unmanned Aerial Vehicles ,  Sensor Networks ,  Data Collection System ,  Candidate Solutions ,  Power Communication ,  Unmanned Aerial Vehicle Trajectory ,  System Energy Consumption ,  Data Collection Framework ,  Optimization Problem ,  Time Constraints ,  Clustering Algorithm ,  Transmission Power ,  Person Years ,  Deep Reinforcement Learning ,  Battery Capacity ,  Multiple Unmanned Aerial Vehicles ,  Traveling Salesman Problem ,  Memory Constraints ,  Unmanned Aerial Vehicles Networks ,  Received Signal Strength Indicator ,  Power Of Unmanned Aerial Vehicles ,  Sensor Density ,  Objective Of Problem ,  Homogeneous Poisson Point Process ,  Speed Of The Unmanned Aerial Vehicles "
Enhancement in 6G Intelligent Network Technologies using Cognitive Radio Network,https://ieeexplore.ieee.org/document/10430814/,0,Conference Paper,IEEE,2023,"The foundational principles of emerging wireless networks such as 5G, cognitive radio, and the prospective Internet have already integrated a multitude of prerequisites from the realm of the Internet of Things (IoT). The utilization of 6G technology opens up the possibility of conducting surgeries from a remote location. The rapid and efficient transmission of medical data can be enabled by the substantial data rates, minimal latency, and viability inherent in the 6G cognitive radio network, thereby potentially elevating both the quality of healthcare and its accessibility. The enhancement of Quality of Service (QoS) crucially depends on resolving the intricacies associated with routing Secondary Users (SUs) in a multi-hop fashion. To cater to the decentralized nature of Cognitive Radio Networks (CRN) within the context of a 6G-oriented Internet of Things (IoT) infrastructure, it becomes imperative to formulate solutions for routing SUs that do not hinge on cooperative mechanisms. The present routing protocols predominantly focus on defining an interference range for SUs, aimed at mitigating user interference. However, these limitations primarily consider interference with Primary Users (PUs) and do not align with the fulfillment of regulations set forth by the Federal Communication Commission (FCC). To address these challenges, a proactive method employs non-cooperative reinforcement learning techniques to systematically formulate IoT network routing strategies enhanced by 6G and cognitive radio capabilities. This pursuit introduces a novel Q-learning algorithm that modifies the existing Ad-hoc on-Demand Distance Vector Routing protocol (AODV), enhancing the reconfiguration process.","Wireless Networks ,  Cognitive Networks ,  Cognitive Radio ,  Data Rate ,  Service Quality ,  Health Quality ,  Internet Of Things ,  Federal Communications Commission ,  Q-learning Algorithm ,  Secondary Users ,  Frequency Band ,  Communication Network ,  Information And Communication Technologies ,  Unmanned Aerial Vehicles ,  Physical Layer ,  Channel Selection ,  Wireless Sensor Networks ,  Nash Equilibrium ,  Cyber-physical Systems ,  Channel Access ,  Free Channel ,  Mobile Edge Computing ,  Terahertz Communications ,  Non-cooperative Game ,  Terahertz Frequency ,  User Equipment ,  Ad Hoc Networks ,  Radio Access Network ,  Baseline Configuration ,  Low Earth Orbit "
Gradient Monitored Reinforcement Learning for Jamming Attack Detection in FANETs,https://ieeexplore.ieee.org/document/10419341/,0,Journal Article,IEEE,2024,"Unmanned Aerial Vehicles (UAVs) have several military and civilian applications to perform tasks that do not require a central processing unit or human involvement. There are various vulnerable characteristics, alternatively limitations, in UAV systems such as data loss, signals interference, disabling sensors, misleading weapons, cyber attacks, disrupting services, etc. Jamming attack is one of the cyber threats that likely lead to denial of service that often occurs in wireless communication systems like Flying ad hoc networks (FANETs) and Internet of Drones (IoD). Over years, there are several approaches proposed by researchers to detect jamming attacks such as rule-based jamming attack detection mechanism, Bayesian game-theoretic mechanism, IoD-based protection mechanism, communication channel techniques (channel hopping, spectrum spreading, MIMO-based jamming mitigation, coding, etc), delay tolerant networking technique, and cryptographic algorithms, however, these methods were not suitable for jamming detection in UAV environment. The major challenges are on the delivery efficiency, processing time, accuracy, energy consumption, flight distance, and flight autonomy. In this paper, we introduce a method to detect the jamming attack using Reinforcement Learning-based Gradient Monitored (RLGM) mechanism. RLGM maintains safe regions and reduces gradient variance for intended training and this provides a better accuracy of the learning goal. In addition, RLGM achieves prompt training progress and selects precisely the series of parameters required by the network during the training phase. RLGM produces spontaneous derivation of the essential deep network scale over the training process drawing on automatically unvarying trained weights. Our proposed approach outperforms other reinforcement learning methods such as Federated RL, Deep Q Learning (DQL), in addition to non-machine learning based techniques such as GA-AOMDV.","Jamming Attacks ,  Flying Ad Hoc Networks ,  Energy Consumption ,  Unmanned Aerial Vehicles ,  Wireless Communication Systems ,  Denial Of Service ,  Ad Hoc Networks ,  Deep Q-learning ,  Unmanned Aerial Vehicle System ,  Neural Network ,  Deep Neural Network ,  Matrix Elements ,  Long Short-term Memory ,  Network Performance ,  Types Of Mechanisms ,  Base Station ,  Actor Network ,  Heaviside Function ,  Topological Changes ,  Data Packets ,  Vehicular Ad Hoc Networks ,  Global Weight ,  Decision Matrix ,  Gradient Matrix ,  Reinforcement Learning Techniques ,  Federated Learning ,  Mobile Nodes ,  Packet Delivery Ratio ,  Network Lifetime ,  Minimum Energy Consumption "
A bird's eye view on reinforcement learning approaches for power management in WSNs,https://ieeexplore.ieee.org/document/6548988/,5,Conference Paper,IEEE,2013,"This paper presents a survey on the adoption of Reinforcement Learning (RL) approaches for power management in Wireless Sensor Networks (WSNs). The survey has been carried out after a review expressly focused on the most relevant and the most recent contributions for the topic. Moreover, the analysis encompassed proposals at every methodological level, from dynamic power management to adaptive autonomous middleware, from self learning scheduling to energy efficient routing protocols.","Wireless Sensor Networks ,  Bird’s Eye ,  Power Management ,  Reinforcement Learning Approach ,  Energy Efficiency ,  Dynamic Management ,  Energy Consumption ,  Service Quality ,  Active Period ,  Duty Cycle ,  Energy Harvesting ,  Time Slot ,  Fault-tolerant ,  Reward Function ,  Ultra-wideband ,  Cooperative Model ,  Transmission Parameters ,  Reinforcement Learning Model ,  Queue Length ,  Classical Formula ,  Greedy Policy ,  Network Lifetime ,  Cluster Head ,  Routing Path ,  Exploitation And Exploration ,  Energy Harvesting Devices ,  Routing Table ,  Solar Panels ,  Global Optimization ,  Adaptive Management "
Reliable and Low-Latency Routing Mechanism for IoT-based Networks,https://ieeexplore.ieee.org/document/9037495/,2,Conference Paper,IEEE,2019,"Routing discovery is an essential component of the communication stack in low-power and lossy networks (LLNs). IPv6 routing protocol for LLNs, termed RPL, has been recently standardized for routing in LLNs leading to the Internet of Things (IoT) concept. RPL is designed with an assumption of gathering data towards a single destination, typically the root node. On the other hand, the arbitrary communication, where neither the source nor the destination is the root, is a prime requirement in most of LLNs-based actuating, decision making, and controlling applications. In this paper, we propose a reliable and low-latency routing mechanism based on RPL for high arbitrary communications in LLNs that is referred to herein by RL-RPL. RL-RPL utilizes the original RPL control features to guarantee backward-compatibility with RPL specifications. Using the cooja emulator as a platform, RL-RPL shows enhanced packet delivery ratio relative to the standard routing mechanism. In addition, it shows an average 74% reduction in delay together with an average 21.3% reduction in energy consumption across the considered traffic intensities.","ordsInternet of Things ,  low-power lossy network ,  wireless sensor network ,  RPL ,  routing ,  load balancing ,  IEEE 802.15.4 "
An Optimized Stochastic Distance Vector (OSDV) Protocol for Enhanced Energy Efficiency in Wireless Sensor Networks,https://ieeexplore.ieee.org/document/10395216/,0,Conference Paper,IEEE,2023,"In recent years, Wireless Sensor Networks (WSNs) are attracting a great deal of interest from both the business world and the academic community. WSNs are often made up of a network of Sensor Nodes (SNs), which perform various tasks, such as data collecting, interpreting, and transmitting. While batteries have been employed for powering WSNs across numerous commercial uses, their limited lifespan and cumbersome replacement need have evolved into major obstacles to the widespread adoption of WSNs. Managing energy throughout a WSN's lifecycle is crucial. In this research a new Optimized Stochastic Distance Vector (OSDV) protocol has been proposed to improve the energy efficiency of WSN using a distance metric, which is the focus of this research. To determine the Sink Node (SiN) distance, it compare the initial distance from a fixed point to the final distance needed for reaching the SiN. Data packets are sent using this protocol following an index of neighboring SN metrics. To improve the efficiency of the network, the suggested OSP prioritizes reducing energy consumption and redistributing traffic between SNs. The network's expected lifetime will increase as a result. Finally it compare the efficiency of the OSDV proposed protocol with the DMSSS current protocol concerning the metrics of “Energy Efficiency”, “Packet Delivery Ratio (PDR)”, “Throughput”, and “Routing Overhead” over the WSN.","Energy Efficiency ,  Wireless Sensor Networks ,  Throughput ,  Data Packets ,  Sink Node ,  Packet Delivery Ratio ,  Data Transmission ,  Highest Probability ,  Deep Reinforcement Learning ,  Response Information ,  Flat Structure ,  Routing Path ,  Lower Limit Value ,  Data Packet Transmission ,  Hop Count "
Distributed Multi-Hop Traffic Engineering via Stochastic Policy Gradient Reinforcement Learning,https://ieeexplore.ieee.org/document/9013134/,3,Conference Paper,IEEE,2019,"Multi-hop networks (e.g., mesh, ad-hoc, and sensor networks) are important and cost-efficient communication backbones. Over the last few years wireless data traffic has drastically increased due to the changes in the way today's society creates, shares, and consumes information. This demands the efficient and intelligent utilization of limited network resources to optimize network performance. Traffic engineering (TE) optimizes network performance and enables optimal forwarding and routing rules to meet the quality of service (QoS) requirements for a large volume of traffic flows. This paper proposes a distributed model-free TE solution based on stochastic policy gradient reinforcement learning (RL), which aims to learn a stochastic routing policy for each router so that each router can send a packet to the next-hop router according to the learned optimal probability. The proposed policy-gradient solution naturally leads to multi-path TE strategies, which can effectively distribute the high traffic loads among all available routing paths to minimize the E2E delay. Moreover, a distributed software-defined networking architecture is proposed, which enables the fast prototyping of the proposed multi-agent actor-critic TE (MA-AC TE) algorithm and in-nature supports automated TE through multi-agent RL learning.","Stochastic Gradient ,  Policy Gradient ,  Traffic Engineering ,  Stochastic Policy Gradient ,  Service Quality ,  Sensor Networks ,  Traffic Flow ,  National Science Foundation ,  Traffic Load ,  Society Today ,  Quality Of Service Requirements ,  Routing Path ,  Multi-agent Reinforcement Learning ,  Multi-hop Networks ,  US National Science Foundation ,  Data Rate ,  Network Topology ,  Local Control ,  Local Policy ,  Network State ,  Policy Learning ,  Markov Decision Process ,  Model-based Optimization ,  Policy Parameters ,  Internet Traffic ,  Packet Loss Rate ,  Signal-to-interference-plus-noise Ratio ,  Optimal Policy ,  Policy Targets ,  Heuristic Approach "
Software-Defined Routing Strategy Based on Reinforcement Learning in Smart Power Grid,https://ieeexplore.ieee.org/document/9764077/,3,Conference Paper,IEEE,2022,"To enable a set of advanced functionalities, the smart power grid relies on pervasive sensors and actuators for continuous monitoring and control. These field devices handle both regular and event-driven (emergency) data packets, which require different levels of quality-of-service (QoS) support (latency requirements). Unfortunately, the existing routing strategies in smart power grids do not guarantee differentiated QoS support that adapts to the network dynamics. To address this limitation, we propose a QoS-aware routing strategy in smart grids based on a software-defined network (SDN). The proposed framework establishes separate queues for event-driven and fixed-scheduling packets. Then, the routing strategy is designed based on reinforcement learning to enable adaptive QoS support. In this regard, updating the routing tables at the OpenFlow switches for each incoming packet is not practical and incurs additional delays that would violate the target QoS. Instead, the proposed routing strategy relies on two Q-learning agents that fix an optimal route for each source-destination pair and update the queue service rate for each switch along the route depending on the network condition (packet arrival rate). Our simulation results demonstrate that the proposed adaptive strategy offers effective QoS support compared with an SDN benchmark routing strategy based on Bellman-Ford. For instance, the average percentage of packets that violate their latency requirement is only 3% compared with at least 30% for the benchmark routing strategy.","Power Grid ,  Smart Grid ,  Routing Scheme ,  Actuator ,  Service Rate ,  Pathfinding ,  Data Packets ,  Arrival Rate ,  Latency Requirements ,  Packet Arrival ,  Center For Control ,  Time Slot ,  Average Loss ,  Low Latency ,  Model-based Approach ,  Reward Function ,  Physical Layer ,  Emergency Events ,  Average Latency ,  Packet Loss ,  Reinforcement Learning Agent ,  Packet Loss Rate ,  Phasor Measurement Units ,  Traffic Types ,  Traffic Load ,  Quality Of Service Requirements ,  Traffic Patterns ,  Power Nodes ,  Average Loss Rate ,  Routing Path "
Energy-Efficient IoT with Deep Learning: Optimizing Resource Allocation in Smart Grids,https://ieeexplore.ieee.org/document/10407067/,4,Conference Paper,IEEE,2023,"The integration of Internet of Things (IoT) technology with deep reinforcement learning (DRL) has emerged as a transformative approach in the realm of smart grid management. This abstract explores the intersection of these two domains, focusing on their collective potential to enhance energy efficiency through optimized resource allocation in smart grids. Smart grids represent a pivotal innovation in the modern energy landscape, offering a dynamic platform for the integration of renewable energy sources, efficient load management, and real-time grid monitoring. However, the complex and ever-changing nature of smart grids poses challenges in effectively allocating energy resources to meet diverse consumer demands while minimizing waste. Deep reinforcement learning, a subset of artificial intelligence, harnesses the power of IoT-connected devices and sensors to create adaptive, data-driven resource allocation strategies. The adaptability of DRL models to evolving grid conditions and consumer preferences enables dynamic resource allocation. This adaptability leads to more efficient energy resource utilization, waste reduction, and operational cost savings. Beyond cost benefits, energy-efficient resource allocation contributes significantly to sustainability objectives and grid resilience. The method consistently achieves the highest network throughput, ranging from 2700 units with 100 sensors to 2300 units with 500 sensors.Our model sheds light on the promising synergy between IoT technology and deep reinforcement learning for the purpose of resource allocation optimization in smart grids. The resulting improvements in energy efficiency promise economic advantages while simultaneously advancing environmental sustainability goals.","Deep Learning ,  Resource Allocation ,  Internet Of Things ,  Smart Grid ,  Optimal Resource Allocation ,  Energy-efficient Internet Of Things ,  Energy Source ,  Energy Efficiency ,  E-learning ,  Deep Reinforcement Learning ,  Network Throughput ,  Internet Of Things Technology ,  Neural Network ,  Machine Learning ,  Data Management ,  Data Transmission ,  Global Positioning System ,  Internet Of Things Devices ,  Energy Usage ,  Wireless Sensor Networks ,  Cache Hit ,  Internet Of Things Sensors ,  Sink Node ,  Smart Meters ,  Smart Sensors ,  Content Popularity ,  Internet Of Vehicles ,  Edge Devices ,  Routing Decisions ,  Energy Meter "
Intelligent Parent Change to Improve 6TiSCH Network Transmission Using Multi-Agent Q-Learning,https://ieeexplore.ieee.org/document/10609377/,0,Journal Article,IEEE,2024,"The 6TiSCH (IPv6 over the TSCH mode of IEEE 802.15.4e) architecture for wireless sensor networks merges the time-slotted channel hopping (TSCH) at the medium access control (MAC) layer with the routing protocols tailored for low-power and lossy networks (RPL). However, research often neglects the incorporation between TSCH MAC and RPL. Standard RPL strategies rely on an objective function (OF) using the expected transmission count (ETX) metric, which does not adequately reflect the traffic dynamics. Moreover, RPL’s hysteresis function employs a static threshold to control parent change decisions. This static setting disregarded the diverse traffic patterns within the network, leading to unnecessary parent node changes and preventing the node from selecting a better parent. To overcome these shortcomings, we introduce 3 advancements to standard RPL. First, an adaptive parent-changing mechanism based on cooperative Q-learning. Second, a cell usage and traffic load aware objective function. Third, an improved initial transmission cell allocation. Those methods are collectively termed ACI-RPL. We evaluated the performance of the proposed method through simulations using the 6TiSCH simulator and real-hardware tests on the FIT IoT-Lab testbed with OpenWSN firmware. The experiment result indicates that ACI-RPL performs better than the benchmark algorithms. In comparison to the standard RPL, ACI-RPL improves the packet delivery ratio and the total received packets by 12% and 17%, respectively. Additionally, ACI-RPL reduces energy consumption and latency by 23% and 9%.","6TiSCH Network ,  Energy Consumption ,  Objective Function ,  Root Node ,  Wireless Sensor Networks ,  Traffic Load ,  Benchmark Algorithms ,  Medium Access Control ,  Medium Access ,  Medium Access Control Layer ,  Packet Delivery Ratio ,  Static Threshold ,  High Use ,  Changes In Frequency ,  Internet Of Things ,  Network Size ,  Traffic Conditions ,  Markov Decision Process ,  Directed Acyclic Graph ,  Child Nodes ,  Quality Of Transmission ,  Packet Transmission ,  Control Packets ,  Packet Drop ,  Candidate Parents ,  Multi-agent Reinforcement Learning ,  Selection Of Parents ,  Hop Count ,  Comprehensive Metrics ,  Traffic Rate "
Deep Reinforcement Learning for Modulation and Coding Scheme Selection in Cognitive HetNets,https://ieeexplore.ieee.org/document/8761663/,6,Conference Paper,IEEE,2019,"We study a cognitive heterogeneous network (HetNet), in which multiple pairs of secondary users coexist with a pair of primary users on a certain spectrum band. To protect primary transmissions, secondary transmitters (STs) adopt a sensing-based approach to access the spectrum band. Nevertheless, STs may cause uncertain interference to the primary receiver (PR) due to imperfect spectrum sensing, which is particularly significant when the wireless links between the primary transmitter (PT) and STs are extremely weak and the wireless links between STs and the PR are non-ignorable. This makes it difficult for the PR to select a proper modulation and/or coding scheme (MCS). To deal with the issue, we propose an intelligent deep reinforcement learning (DRL) based MCS selection algorithm for the primary transmission. With the proposed algorithm, the DRL agent at the PR is able to learn the pattern of the interference from the STs and predict the interference in the future. Simulation results show that the transmission rate of the proposed algorithm can converge to 90% ^ 100% transmission rate of the optimal MCS selection algorithm, which assumes that the interference from the STs is perfectly known at the PR as prior information. Meanwhile, the transmission rate of the proposed algorithm is around 100% higher than the transmission rate of the benchmark algorithm, which selects the MCS without the information about interference.","Deep Reinforcement Learning ,  Optimization Algorithm ,  Transmission Rate ,  Spectral Bands ,  Selection Algorithm ,  Optimal Selection ,  Cognitive Networks ,  Wireless Link ,  User Pairing ,  Secondary Users ,  Deep Reinforcement Learning Agent ,  Environmental Conditions ,  State Space ,  Wireless Networks ,  Optimal Policy ,  Intelligence Algorithms ,  Reward Function ,  Channel Gain ,  Local Memory ,  Successive Phases ,  Rayleigh Fading ,  Long-term Reward ,  Deep Reinforcement Learning Algorithm ,  Optimal Action ,  Decision-making Scenarios ,  Symbol Error Rate ,  Transmission Frame ,  Uplink Data ,  Experience Replay ,  Previous Frame "
Contention Window-Based MAC Protocol for Wireless Sensor Networks,https://ieeexplore.ieee.org/document/6945737/,1,Conference Paper,IEEE,2014,"Existing duty-cycle MAC protocols for wireless sensor networks(WSNs), such as S-MAC, mostly do not consider the influence of inherent traffic distribution characteristic derived from network infrastructure, leading to significant end-to-end delivery latency and poor medium contention handling in data gathering. To mitigate these drawbacks, we propose a novel duty-cycle MAC protocol, called RL-MAC (location-based RMAC). Firstly RL-MAC exploits a location-based configuration strategy to configure nodes with un-uniform CWmin (minimum contention window) values during network initialization, in order to optimize application-level medium access fairness, avoiding packet loss and buffer overflow incurred by mismatches between un-uniform payload distribution over nodes in different position and node-level equal medium access scheme adopted by current protocols. Secondly like RMAC, RL-MAC can forward data packets over multiple hops per cycle through intermediate nodes according to cross-layer routing information, significantly reducing end-to-end delivery latency. Our simulation results obtained on ns-2 show that RL-MAC can better accommodate data gathering applications, providing high throughput and packet delivery ratio without sacrificing energy efficiency.","Wireless Sensor Networks ,  MAC Protocol ,  Energy Efficiency ,  Per Cycle ,  Network Infrastructure ,  Data Packets ,  Packet Loss ,  Medium Access ,  Packet Delivery ,  Minimum Window ,  Packet Delivery Ratio ,  Buffer Overflow ,  Configuration Scheme ,  Energy Consumption ,  Computational Details ,  Random Selection ,  Adjustment For Factors ,  Duty Cycle ,  Time Slot ,  Cycling Performance ,  Control Packets ,  Traffic Load ,  Sleep Schedule ,  Time Synchronization ,  Packet Size ,  Packet Forwarding "
QELAR: A Machine-Learning-Based Adaptive Routing Protocol for Energy-Efficient and Lifetime-Extended Underwater Sensor Networks,https://ieeexplore.ieee.org/document/5408367/,0,Journal Article,IEEE,2010,"Underwater sensor network (UWSN) has emerged in recent years as a promising networking technique for various aquatic applications. Due to specific characteristics of UWSNs, such as high latency, low bandwidth, and high energy consumption, it is challenging to build networking protocols for UWSNs. In this paper, we focus on addressing the routing issue in UWSNs. We propose an adaptive, energy-efficient, and lifetime-aware routing protocol based on reinforcement learning, QELAR. Our protocol assumes generic MAC protocols and aims at prolonging the lifetime of networks by making residual energy of sensor nodes more evenly distributed. The residual energy of each node as well as the energy distribution among a group of nodes is factored in throughout the routing process to calculate the reward function, which aids in selecting the adequate forwarders for packets. We have performed extensive simulations of the proposed protocol on the Aqua-sim platform and compared with one existing routing protocol (VBF) in terms of packet delivery rate, energy efficiency, latency, and lifetime. The results show that QELAR yields 20 percent longer lifetime on average than VBF.","ordsRouting protocols ,  distributed networks ,  wireless communication ,  mobile communication systems. "
An SDN-Enabled Framework for a Load-Balanced and QoS-Aware Internet of Underwater Things,https://ieeexplore.ieee.org/document/9997108/,12,Journal Article,IEEE,2023,"The massive demand for marine exploitation has promoted the thriving Internet of Underwater Things (IoUT). The volume, velocity, and variety (3V) of data produced by sensors, hydrophones, and cameras in IoUT are enormous, which challenges the network in achieving load balancing and Quality-of-Service (QoS) provisioning. This article adopts the “SDN+AI” paradigm to realize a load-balanced and QoS-aware software-defined IoUT from a framework design. We first introduce SDN technology to separate the data plane from the control plane to enhance the network’s scalability and flexibility. Then, a multicontroller load-balancing strategy based on switch migration called CASM is proposed to improve the network’s performance further. With the global view provided by SDN controllers, we proposed a QoS-aware adaptive routing protocol (SQAR) based on reinforcement learning, which can intelligently select route paths to satisfy the QoS requirements of multiple IoUT services. The results show that CASM achieves an efficient load balance while shortening the response time and average control path latency of the switch migration process, which significantly benefits our routing protocol. SQAR outperforms the existing QoS-aware routing protocols regarding QoS satisfaction probability, energy consumption, and convergence rate. Overall, our framework maintains a QoS violation rate below 5% and a load-balancing rate above 90% in a timely manner.","Load Balancing ,  Response Time ,  Convergence Rate ,  Version Of Protocol ,  Average Latency ,  Quality Of Service Requirements ,  Control Plane ,  Routing Path ,  Learning Rate ,  Softmax ,  Shortest Path ,  Flow Data ,  Adjustable Parameters ,  Time Efficiency ,  Reward Function ,  Markov Decision Process ,  Transmission Distance ,  Flow Set ,  Distributed Architecture ,  Autonomous Underwater Vehicles ,  Master Controller ,  Routing Decisions ,  Global Topology ,  Packet Delivery Ratio ,  Load Imbalance ,  Flow Table ,  Link Bandwidth ,  Energy Of Nodes ,  Temperature Coefficient "
DRL-UTPS: DRL-Based Trajectory Planning for Unmanned Aerial Vehicles for Data Collection in Dynamic IoT Network,https://ieeexplore.ieee.org/document/9916069/,27,Journal Article,IEEE,2023,"Using highly maneuverable Unmanned Aerial Vehicles (UAV) to collect data is a fast and efficient method that is widely studied. In most studies, they assume that the UAVs can obtain the location of the Cluster Head (CH) before take-off, allocate CHs, and optimize the trajectory in advance. However, in many real scenarios, many sensing devices are deployed in areas with no basic communication infrastructure or cannot communicate with the Internet due to emergencies such as disasters. In this kind of sensing network, the surviving devices often change, and the CHs cannot be known and allocated in advance, thus bringing new challenges to the efficient data collection of the networks by using UAVs. In this paper, a UAV path planning scheme for IoT networks based on reinforcement learning is proposed. It plans hover points for UAV by learning the historical location of CHs and maximizes the probability of meeting CHs and plans the shortest UAV path to visit all hover points by using the simulated annealing method. In addition, an algorithm to search for the location of CHs is proposed which is named Cluster-head Searching Algorithm with Autonomous Exploration Pattern (CHSA-AEP). By using CHSA-AEP, our scheme enables the UAV to respond to the position change of the CHs. Finally, we compare our scheme with other algorithms (area coverage algorithms and random algorithm). It is found that our proposed scheme is superior to other methods in energy efficiency and time utilization ratio.","Unmanned Aerial Vehicles ,  Trajectory Planning ,  IoT Networks ,  Disaster ,  Energy Efficiency ,  Shortest Path ,  Simulated Annealing ,  Path Planning ,  Cluster Head ,  Energy Consumption ,  Time Slot ,  Reduce Energy Consumption ,  Network Clustering ,  Internet Of Things Devices ,  Reward Function ,  Total Energy Consumption ,  Deep Reinforcement Learning ,  Markov Decision Process ,  Wireless Sensor Networks ,  Non-line-of-sight ,  Unmanned Aerial Vehicle Trajectory ,  Proximal Policy Optimization ,  Traveling Salesman Problem ,  Network Devices ,  Flight Trajectory ,  Unmanned Aerial Vehicle Flight ,  Simulated Annealing Algorithm ,  Flight Path ,  Unmanned Aerial Vehicle Flies ,  Dynamic Routing "
Finding MARLIN: Exploiting multi-modal communications for reliable and low-latency underwater networking,https://ieeexplore.ieee.org/document/8057132/,35,Conference Paper,IEEE,2017,"This paper concerns the smart exploitation of multimodal communication capabilities of underwater nodes to enable reliable and swift underwater networking. To contrast adverse and highly varying channel conditions we define a smart framework enabling nodes to acquire knowledge on the quality of the communication to neighboring nodes over time. Following a model-based reinforcement learning approach, our framework allows senders to select the best forwarding relay for its data jointly with the best communication device to reach that relay. We name the resulting forwarding method MARLIN, for MultimodAl Reinforcement Learning-based RoutINg. Applications can choose whether to seek reliable routes to the destination, or whether faster packet delivery is more desirable. We evaluate the performance of MARLIN in varying networking scenarios where nodes communicate through two acoustic modems with widely different characteristics. MARLIN is compared to state-of-the-art forwarding protocols, including a channel-aware solution, a machine learning-based solution and to a flooding protocol extended to use multiple modems. Our results show that a smartly learned selection of relay and modem is key to obtain a packet delivery ratio that is twice as much that of other protocols, while maintaining low latencies and energy consumption.","Multimodal Communication ,  Energy Consumption ,  Neighboring Nodes ,  Low Latency ,  Communication Devices ,  Fast Delivery ,  Packet Delivery ,  Model-based Reinforcement Learning ,  Packet Delivery Ratio ,  Data Rate ,  Value Function ,  Cost Function ,  State Space ,  Transition Probabilities ,  Network Size ,  Optimal Policy ,  Multiple Devices ,  Final Destination ,  Wireless Sensor Networks ,  Autonomous Underwater Vehicles ,  Data Packets ,  Packet Collisions ,  Packet Header ,  Bitrate ,  Packet Loss ,  Packet Transmission ,  Control Packets ,  Residual Energy ,  Most Significant Bit "
Machine-Learning-Based Optimal Cooperating Node Selection for Internet of Underwater Things,https://ieeexplore.ieee.org/document/10479225/,1,Journal Article,IEEE,2024,"Multihop communication has gained prominence within the realm of the Internet of Underwater Things (IoUT) owing to its exceptional reliability amidst the challenges posed by the underwater acoustic environment. Despite this, the persistence of limitations caused by propagation delay, high collision rate, and limited energy in underwater communication remains, representing the most formidable hurdles in ensuring the successful transmission of data gathered by sensor nodes. To address these challenges, we employ a machine learning (ML)-based optimal cooperating node selection for each hop, considering the Shortest propagation delay, minimal residual Energy, and a low Collision rate (referred to as SEC). For this purpose, we initially assemble the sensor nodes to create a list of cooperative nodes, considering the aspect of SEC. Then, using an assembled list of cooperating sensor nodes, we employ ML-based algorithms, such as reinforcement learning (RL-SEC), deep Q-networks (DQN-SEC), and deep deterministic policy gradient (DDPG-SEC), to predict the optimal cooperating node for each hop. The simulation results of the DDPG-SEC demonstrate a significant improvement of approximately 56% when compared with RL-SEC, DQN-SEC, and other state-of-the-art techniques.","Optimal Selection ,  Optimal Node ,  Policy Learning ,  Successful Transmission ,  Propagation Delay ,  Residual Energy ,  Policy Gradient ,  Deep Q-network ,  List Of Nodes ,  Underwater Communication ,  Reinforcement Learning Policy ,  Multi-hop Communication ,  Energy Consumption ,  Average Energy ,  Neighboring Nodes ,  Bit Error Rate ,  Real-world Environments ,  Markov Decision Process ,  Target Network ,  Data Packets ,  Destination Node ,  Undersea ,  Policy Gradient Method ,  Successful Transmission Probability ,  Network Throughput ,  Source Node ,  Primary Network ,  Control Packets ,  Arrival Rate ,  Replay Buffer "
Traffic-Aware Configuration of All-Optical Data Center Networks Based on Hyper-FleX-LION,https://ieeexplore.ieee.org/document/10433414/,0,Journal Article,IEEE,2024,"Due to the advantages of optical circuit switching (OCS), all-optical data center networks (DCNs) have attracted intensive research interests recently. Hyper-FleX-LION is a highly-flexible all-optical DCN architecture that operates with the OCS based on wavelength-division multiplexing (WDM). In this work, we study how to realize traffic-aware configuration of all-optical DCNs in Hyper-FleX-LION. We formulate an integer linear programming (ILP) model for the problem to jointly optimize the configuration of Hyper-FleX-LION and the provisioning schemes of demands in it for minimizing its port usage. To ensure the practicalness of the optimization, we assume that each top-of-rack (ToR) switch can not only receive the traffic targeting to its rack but also forward traffic to other racks as an intermediate node. We also classifier traffic demands as normal and latency-sensitive ones, and set the maximum hop-count for routing latency-sensitive demands. By analyzing the complexity of the problem theoretically, we prove its 
 $\mathcal {APX}$ 
-hardness, i.e., there does not exist a polynomial-time approximation algorithm for it unless 
 $\mathcal {P}=\mathcal {NP}$ 
. Then, we propose a polynomial-time heuristic JTRO based on iterative optimization to solve the problem effectively and time-efficiently. Extensive numerical simulations verify the effectiveness of our proposed algorithm. We also build a small-scale but real all-optical DCN testbed in Hyper-FleX-LION to interconnect four racks, and leverage distributed machine learning (DML) as the network services in it to demonstrate the performance of our proposal experimentally.","Data Center Networks ,  Heuristic ,  Hardness ,  Model Problem ,  Network Services ,  Iterative Optimization ,  Polynomial-time Algorithm ,  Linear Programming Model ,  Optical Switching ,  Traffic Demand ,  Wavelength Division Multiplexing ,  Integer Linear Programming Model ,  Distributed Machine Learning ,  Convolutional Neural Network ,  Feasible Solution ,  Directed Graph ,  Specific Instances ,  Deep Reinforcement Learning ,  Traffic Load ,  Topology Optimization ,  Traffic Routes ,  Job Completion Time ,  Master Node ,  Topology Design ,  Types Of Demands ,  Receptor For Activated C Kinase 1 ,  Bandwidth Usage ,  Worker Nodes ,  Routing Path ,  Routing Scheme "
Packet Delivery Maximization Using Deep Reinforcement Learning-Based Transmission Scheduling for Industrial Cognitive Radio Systems,https://ieeexplore.ieee.org/document/9585683/,0,Journal Article,IEEE,2021,"The performance of data aggregation in industrial wireless communications can be degraded by environmental interference on Industrial Scientific Medical (ISM) channels. In this paper, cognitive radio (CR) was applied to enable devices to share primary channels with the aim of enhancing the transmission performance of the WirelessHART network. We considered a linear convergecast system, where the packets generated at each device were routed to the gateway (GW) through the aid of neighboring devices. The solar-powered cognitive access points (CAPs) were deployed to improve the network performance by opportunistically allocating the primary channels to the devices for data transmissions. Firstly, we formulate the scheduling problem of long-term throughput maximization as a framework of a Markov decision process with the constraints of the minimum delay, the number of required ISM channels, and the harvested energy at the CAPs. Then, we propose a deep reinforcement learning-based scheduling scheme to optimally assign multiple ISM and primary channels to the field devices in each superframe. The simulation results confirmed the superiority of the proposed scheme compared to existing methods.","Cognitive Radio ,  Packet Delivery ,  Transmission Scheduling ,  Data Transmission ,  Network Performance ,  Energy Harvesting ,  Markov Decision Process ,  Primary Channel ,  Deep Learning ,  Probability Values ,  Wireless Networks ,  Flow Data ,  False Alarm ,  Time Slot ,  Optimal Policy ,  Channel Flow ,  Deep Reinforcement Learning ,  Wireless Sensor Networks ,  Data Packets ,  Cognitive Networks ,  Industrial Networks ,  Deep Q-learning ,  Solar Energy Harvesting ,  Replay Memory ,  System Throughput ,  Joint Data ,  Network Throughput ,  Successful Transmission Probability ,  Linear Function ,  Dynamic Channel "
Reinforcement Learning for Optimal Control of Queueing Systems,https://ieeexplore.ieee.org/document/8919665/,12,Conference Paper,IEEE,2019,"With the rapid advance of information technology, network systems have become increasingly complex and hence the underlying system dynamics are typically unknown or difficult to characterize. Finding a good network control policy is of significant importance to achieving desirable network performance (e.g., high throughput or low average job delay). Online/sequential learning algorithms are well-suited to learning the optimal control policy from observed data for systems without the information of underlying dynamics. In this work, we consider using model-based reinforcement learning (RL) to learn the optimal control policy of queueing networks so that the average job delay (or equivalently the average queue backlog) is minimized. Existing RL techniques, however, cannot handle the unbounded state spaces of the network control problem. To overcome this difficulty, we propose a new algorithm, called Piecewise Decaying ε-Greedy Reinforcement Learning (PDGRL), which applies model-based RL methods over a finite subset of the state space. We establish that the average queue backlog under PDGRL with an appropriately constructed subset can be arbitrarily close to the optimal result. We evaluate PDGRL in dynamic server allocation and routing problems. Simulations show that PDGRL minimizes the average queue backlog effectively.","Optimal Control ,  Queueing System ,  System Dynamics ,  State Space ,  Network System ,  Control Network ,  Optimal Policy ,  Reinforcement Learning Methods ,  Routing Problem ,  Dynamic Allocation ,  Model-based Reinforcement Learning ,  Complex Systems ,  Upper Bound ,  Cost Function ,  Performance Metrics ,  Time Slot ,  Lyapunov Function ,  Markov Decision Process ,  Wireless Sensor Networks ,  Policy Learning ,  Unknown Dynamics ,  Model-based Framework ,  State Transition Function ,  Arbitrary Topology ,  Arrival Rate ,  Auxiliary System ,  Probability Pi ,  Central Server ,  Exploration Stage "
Deep reinforcement learning-based collaborative routing algorithm for clustered MANETs,https://ieeexplore.ieee.org/document/10091930/,2,other,IEEE,2023,"Flexible adaptation to differentiated quality of service (QoS) is quite important for future 6G network with a variety of services. Mobile ad hoc networks (MANETs) are able to provide flexible communication services to users through self-configuration and rapid deployment. However, the dynamic wireless environment, the limited resources, and complex QoS requirements have presented great challenges for network routing problems. Motivated by the development of artificial intelligence, a deep reinforcement learning-based collaborative routing (DRLCR) algorithm is proposed. Both routing policy and subchannel allocation are considered jointly, aiming at minimizing the end-to-end (E2E) delay and improving the network capacity. After sufficient training by the cluster head node, the Q-network can be synchronized to each member node to select the next hop based on local observation. Moreover, we improve the performance of training by considering historical observations, which can improve the adaptability of routing policies to dynamic environments. Simulation results show that the proposed DRLCR algorithm outperforms other algorithms in terms of resource utilization and E2E delay by optimizing network load to avoid congestion. In addition, the effectiveness of the routing policy in a dynamic environment is verified.","ordsartificial intelligence ,  deep reinforcement learning ,  collaborative routing ,  MANETs ,  6G "
