title,link,number_of_citation,article_type,publisher,publication_date,abstract,keyword
New Modified Adaptive Learning Neural Network for Coverage and Connectivity Issues in WSN Using Energy Efficient Routing Algorithm,https://ieeexplore.ieee.org/document/10235038/,0,Conference Paper,IEEE,2023,"Wireless Sensor Networks (WSNs) are increasingly being employed in a variety of applications ranging from military to infrastructure monitoring. However, with the ever-increasing demand of higher coverage, better connectivity, and accuracy, optimization of such networks is becoming a complex task. To address this challenge, this paper proposes a novel Modified Adaptive Learning Neural Network (M-ALNN).This new method is designed to address both coverage and connectivity issues within a given WSN. The MALNN adaptively optimizes WSNs by taking advantage of state-of-the-art reinforcement learning algorithms. The proposed algorithm uses a combination of two different algorithms, viz., the Supervised Neuron Algorithm (SNA) and the K-Means Clustering Algorithm, to achieve optimal coverage and reconfiguration of network topology. Both SNA and K-Means Algorithm are trained through a reinforcement learning algorithm to adaptively adjust their parameters as per the current application requirements. After optimization, the proposed M-ALNN algorithm improves the coverage and connectivity of WSNs by adjusting the network parameters. Simulation experiments are conducted to verify the effectiveness of the proposed method. Results show that our proposed network outperforms the conventional WSN in terms of coverage and connectivity when tested under various scenarios.","Neural Network ,  Energy Efficiency ,  Adaptive Learning ,  Wireless Sensor Networks ,  Routing Algorithm ,  Connectivity Issues ,  Adaptive Neural Network ,  Coverage Issues ,  Energy-efficient Routing Algorithm ,  Learning Algorithms ,  Sensor Networks ,  K-means Algorithm ,  Optimal Coverage ,  Environmental Changes ,  Artificial Neural Network ,  Internet Of Things ,  Network Performance ,  Wireless Networks ,  Faster Convergence ,  Coverage Levels ,  Type Of Neural Network ,  Artificial Neural Network Algorithm ,  Self-organizing Map ,  Coverage Problem ,  Node Connectivity ,  Connectivity Problems ,  Fuzzy Logic ,  Wireless Technologies ,  Solutions For Issues ,  Nodes In Order "
Learning-Based Terminal-Edge Collaborative Energy-Efficient Routing Algorithm for Green RWSN,https://ieeexplore.ieee.org/document/10490003/,0,Journal Article,IEEE,2024,"In recent years, wireless smart sensors powered by solar energy have been widely deployed to ensure the green and sustainable operation of remote industrial systems monitoring. Such devices can offload computing tasks locally or using the edge server by transmitting the raw data wirelessly. Since random renewable energy harvesting has a detrimental effect on the energy balance of nodes in these green rechargeable wireless sensor networks (RWSN), the rational synergy between terminal-edge collaborative tasks offloading (TECTO) and network topology optimization (NTO) is of great significance for improving the sustainability. Therefore, this article presents a learning-based terminal-edge collaborative energy-efficient routing algorithm. First, a system model is developed to integrate TECTO and NTO, and the original problem is decoupled into two layers. Then, the NTO layer is aimed at quickly generating an energy-efficient network topology by variable cycle block coordinate descent method based on the greedy strategy. Finally, the TECTO layer adopts deep reinforcement learning based on the dynamic baseline to understand the energy efficiency feedback law of the NTO layer and rationally adjusts the TECTO scheme. The simulation results show that the presented algorithm can reasonably generate the network topology and TECTO scheme according to the node's energy state change and efficiently consume the renewable energy distributed in the green RWSN, which significantly enhances its sustainability.","Routing Algorithm ,  Energy-efficient Routing ,  Wireless Rechargeable Sensor Networks ,  Energy-efficient Routing Algorithm ,  Raw Data ,  Model System ,  Deep Learning ,  Renewable Energy ,  Energy Efficiency ,  Energy Balance ,  Network Topology ,  Energy State ,  Solar Energy ,  Energy Harvesting ,  Wireless Sensor ,  Deep Reinforcement Learning ,  Wireless Sensor Networks ,  Topology Optimization ,  Edge Server ,  Greedy Strategy ,  Sink Node ,  Optimization Problem ,  Residual Energy ,  Solar Energy Harvesting ,  Raw Sensor Data ,  Edge Computing ,  Internet Of Things ,  Solar Panels ,  Energy Consumption ,  Traffic Model "
Topology-Aware Resilient Routing Protocol for FANETs: An Adaptive Q-Learning Approach,https://ieeexplore.ieee.org/document/9744096/,30,Journal Article,IEEE,2022,"Flying ad hoc networks (FANETs) play a crucial role in numerous military and civil applications since it shortens mission duration and enhances coverage significantly compared with a single unmanned aerial vehicle (UAV). Whereas, designing an energy-efficient FANETs routing protocol with a high packet delivery rate (PDR) and low delay is challenging owing to the dynamic topology changes. In this article, we propose a topology-aware resilient routing strategy based on adaptive 
 $Q$ 
-learning (TARRAQ) to accurately capture topology changes with low overhead and make routing decisions in a distributed and autonomous way. First, we analyze the dynamic behavior of UAVs nodes via the queuing theory, and then the closed-form solutions of neighbors’ change rate (NCR) and neighbors’ change interarrival time (NCIT) distribution are derived. Based on the real-time NCR and NCIT, a resilient sensing interval (SI) is determined by defining the expected sensing delay of network events. Besides, we also present an adaptive 
 $Q$ 
-learning approach that enables UAVs to make distributed, autonomous, and adaptive routing decisions, where the above SI ensures that the action space can be updated in time with low cost. The simulation results verify the accuracy of the topology dynamic analysis model, and also prove that our TARRAQ outperforms the 
 $Q$ 
-learning-based topology-aware routing (QTAR), mobility prediction-based virtual routing (MPVR), and greedy perimeter stateless routing based on energy-efficient hello (EE-Hello) in terms of 25.23%, 20.24%, and 13.73% lower overhead, 9.41%, 14.77%, and 16.70% higher PDR, and 5.12%, 15.65%, and 11.31% lower energy consumption, respectively.","Flying Ad Hoc Networks ,  Energy Consumption ,  Time Distribution ,  Unmanned Aerial Vehicles ,  Low Energy Consumption ,  Topological Changes ,  Queueing System ,  Low Overhead ,  Ad Hoc Networks ,  Resilience Strategies ,  Routing Scheme ,  Higher Rates Of Delivery ,  Role Of The Military ,  Dynamic Topology ,  Inter-arrival Time ,  Packet Delivery ,  Routing Decisions ,  Autonomous Way ,  Rapid Changes ,  Internet Of Things ,  Arrival Rate ,  Reward Function ,  Exploitation And Exploration ,  Service Duration ,  Control Packets ,  Adaptive Learning ,  Relay Selection ,  Neighbor List ,  Reinforcement Learning Techniques ,  Transmission Range "
Full Echo Q-routing with adaptive learning rates: A reinforcement learning approach to network routing,https://ieeexplore.ieee.org/document/7448188/,20,Conference Paper,IEEE,2016,"Dynamically changing networks, such as mobile wireless sensor networks, Internet of Things networks, vehicular ad hoc networks etc., require efficient routing techniques. We present a routing algorithm, Adaptive Q-routing Full Echo, that is an extension of `full echo' modification of Q-routing algorithm and uses adaptive learning rates to improve exploration behaviour. The performance of the proposed algorithm is evaluated empirically in comparison to Q-routing and Dual Q-routing algorithms. The preliminary results suggest that the proposed algorithm represents a promising way of achieving good routing performance in dynamically changing networks.","Learning Rate ,  Reinforcement Learning Approach ,  Adaptive Learning Rate ,  Full Echo ,  Internet Of Things ,  Promising Way ,  Wireless Sensor Networks ,  Ad Hoc Networks ,  Internet Of Things Networks ,  Routing Algorithm ,  Mobile Sensors ,  Vehicular Ad Hoc Networks ,  High Load ,  Low Load ,  Local Information ,  Time Of Delivery ,  Neighboring Nodes ,  High Load Condition ,  Base Learning Rate "
An Energy Efficient Routing Algorithm for WSN Using Q-Learning Based Data Aggregation Method,https://ieeexplore.ieee.org/document/10270449/,0,Conference Paper,IEEE,2023,"Q-Iearning is an energy efficient routing algorithm for wireless sensor networks (WSN s) that uses the reinforcement learning technique for routing decisions. The algorithm is expected to reduce energy consumption in routing by dynamic route selection and maintenance. In Q-Iearning, the energy efficiency of routes is evaluated using a reward system based on the dynamic network properties. Based on these rewards and various learned states, a dynamic Q-table is maintained which is used by the nodes to make routing decisions. To further enhance the energy efficiency, Q-Iearning also supports power saving techniques such as sleep mode that allow the nodes to temporarily reduce their energy consumption. Additionally, Q-Iearning is fault-tolerance aware and utilizes conflict-free transmission techniques to reduce the energy cost and minimize packet loss. The algorithm has been evaluated using the IEEE 802.15.4 standard, and simulation results show that it can reduce energy consumption by up to 53 %, with minimal packet loss.","Energy Efficiency ,  Efficient Algorithm ,  Wireless Sensor Networks ,  Efficient Route ,  Routing Algorithm ,  Energy-efficient Routing ,  Data Aggregation Methods ,  Efficient Routing Algorithms ,  Energy Consumption ,  Dynamic Network ,  Wireless Networks ,  Reward System ,  Reduce Energy Consumption ,  Network Algorithm ,  Dynamic Selection ,  Route Selection ,  Reinforcement Learning Techniques ,  Routing Decisions ,  Environmental Changes ,  Learning Algorithms ,  Pathfinding ,  Optimal Policy ,  Reinforcement Learning Algorithm ,  Optimal Path ,  Load Balancing ,  Q-learning Algorithm ,  Military Applications ,  Base Station ,  Agricultural Applications ,  Markov Decision Process "
Reinforcement Learning Based Routing in Networks: Review and Classification of Approaches,https://ieeexplore.ieee.org/document/8701570/,145,Journal Article,IEEE,2019,"Reinforcement learning (RL), which is a class of machine learning, provides a framework by which a system can learn from its previous interactions with its environment to efficiently select its actions in the future. RL has been used in a number of application fields, including game playing, robotics and control, networks, and telecommunications, for building autonomous systems that improve themselves with experience. It is commonly accepted that RL is suitable for solving optimization problems related to distributed systems in general and to routing in networks in particular. RL also has reasonable overhead-in terms of control packets, memory and computation-compared to other optimization techniques used to solve the same problems. Since the mid-1990s, over 60 protocols have been proposed, with major or minor contributions in the field of optimal route selection to convey packets in different types of communication networks under various user QoS requirements. This paper provides a comprehensive review of the literature on the topic. The review is structured in a way that shows how network characteristics and requirements were gradually considered over time. Classification criteria are proposed to present and qualitatively compare existing RL-based routing protocols.","Control Packets ,  Energy Consumption ,  State Space ,  Wireless Networks ,  Transmission Power ,  Reward Function ,  Optimal Path ,  Wireless Sensor Networks ,  Data Packets ,  Cognitive Networks ,  Source Node ,  Residual Energy ,  Ad Hoc Networks ,  Vehicular Networks ,  Delivery Delay ,  Cluster Head ,  Network Lifetime ,  Secondary Users ,  Routing Table ,  Packet Forwarding ,  Hop Count ,  Average Delay ,  Wavelength Division Multiplexing ,  Learnable Parameters ,  Optical Networks ,  All-to-all Communication ,  Neighboring Nodes ,  Protocol Design ,  Routing Algorithm ,  Game Theory "
A Q-Learning-Based Routing Approach for Energy Efficient Information Transmission in Wireless Sensor Network,https://ieeexplore.ieee.org/document/9933016/,6,Journal Article,IEEE,2023,"Nowadays, wireless sensor networks have played an important role in many applications. In these applications, a large number of wireless sensors are deployed in an environment to collect information and form network to transmit collected information to the base station or sink. Because wireless sensors have to work for a long period of time without maintenance, the energy consumption of wireless sensors has a great impact on the lifetime of wireless sensor network. To reduce and balance the energy consumption of wireless sensors, many information transmission routing approaches have been proposed. However, most of them do not consider all energy consumption factors of wireless sensors. To this end, an innovative information transmission routing approach based on Q-learning is proposed in this paper, which enables wireless sensors to adaptively select suitable neighboring sensors to achieve energy efficient information transmission in a decentralized manner. Based on the proposed approach, a wireless sensor first collects state and action information of its neighboring sensors, which includes information transmission direction and distance, the remaining energy, the energy consumption for information transmission and information transmission action. Then, all this information is used to update the Q-values of neighboring sensors, so as to enable the wireless sensor to select suitable neighboring sensor to transmit information according to Q-values. From simulation experiments, it can be seen that the proposed approach enables wireless sensors to reduce and balance the energy consumption of wireless sensors and extend the lifetime of the entire wireless sensor network.","Energy Efficiency ,  Information Transmission ,  Transmission Efficiency ,  Sensor Networks ,  Wireless Sensor ,  Wireless Sensor Networks ,  Routing Approach ,  Energy Consumption ,  Simulation Experiments ,  Base Station ,  Transmission Distance ,  Network Lifetime ,  Role In Many Applications ,  Suitable Sensor ,  Benchmark ,  Dynamic Changes ,  Center For Control ,  Network Topology ,  General Setting ,  Global View ,  Optimal Path ,  Transmission Time ,  Local View ,  Amount Of Transmission ,  Reward Function ,  Square Of The Distance ,  Deep Reinforcement Learning ,  Routing Algorithm ,  Cluster Head ,  Longest Lifetime "
A Comprehensive Review of an Artificial Intelligence Assisted Resource Allocation Strategy in Wireless Sensor Networks,https://ieeexplore.ieee.org/document/10575724/,0,Conference Paper,IEEE,2024,"In today’s world, the Wireless Sensor Network (WSN) is quite important. The implementation of sensor networks through wireless media has also risen, reflecting the growing importance of tracking and monitoring operations. Sensors are ubiquitous, but they come with a host of problems related to power consumption, security, coverage, latency, and design; WSNs are the subject of active investigation. Extending the useful life of sensors has recently emerged as a priority in the scientific community. With a microelectronic device, a sensor has a limited amount of power. The energy required for processing is supplied by the power source. Charging and upgrading nodes might be challenging task if the nodes are spread apart and the energy usage is just as important as an issue with hardware. This research study enhances the energy consumption of its nodes in WSN and evaluates several methods for localization and resource allocation. This research provides a solution to the challenging problem of node management and scheduling by predicting the energy consumption of WSNs to carry out all communications. By modifying AI logic to increase fault tolerance and including the notion of digital twins to make informed judgments and optimize resource utilization via WSN, this study resolves the prior complications. The findings will be useful in enhancing the knowledge of current procedures and creating new, cutting-edge approaches.","Resource Allocation ,  Sensor Networks ,  Wireless Sensor ,  Wireless Sensor Networks ,  Resource Allocation Scheme ,  Energy Consumption ,  Power Consumption ,  Resource Utilization ,  Local Method ,  Fault-tolerant ,  Energy Usage ,  Digital Twin ,  Optimal Utilization Of Resources ,  Machine Learning ,  Service Quality ,  Energy Efficiency ,  Supervised Learning ,  Internet Of Things ,  Wireless Networks ,  Base Station ,  Energy Consumption Efficiency ,  Reinforcement Learning Algorithm ,  Low Power Wide Area Networks ,  Internet Of Things Networks ,  Network Lifetime ,  Sink Node ,  Edge Computing ,  Routing Algorithm ,  Energy Harvesting ,  Relay Selection "
MARTO: A Multi-agent Deep Reinforcement Learning Based Online Topology Optimization Scheme for Wireless Sensor Network,https://ieeexplore.ieee.org/document/10073203/,1,Conference Paper,IEEE,2022,"Wireless sensor networks (WSNs) have been widely used as a potential network paradigm for the ubiquitous interconnection of devices. However, due to the property of limited energy resources, an energy-efficient routing algorithm is urgently needed to prolong the network lifetime. To tackle this issue, we propose MARTO, a Multi-Agent Deep Reinforcement Learning (MADRL) based scheme to provide online topology optimization, after modeling the WSN lifetime maximization problem as a Decentralized Partially Observable Markov Decision Process (Dec-POMDP). Additionally, a loop-free topology generation algorithm is given which can completely avoid routing loops to assist the decision-making of RL agents. Experimental results show that MARTO outperforms several heuristic algorithms and has high energy efficiency which can effectively prolong the lifetime of WSNs.","Deep Reinforcement Learning ,  Wireless Sensor Networks ,  Topology Optimization ,  Multi-agent Deep Reinforcement Learning ,  Energy Efficiency ,  Heuristic Algorithm ,  Markov Decision Process ,  Routing Algorithm ,  Network Lifetime ,  Energy Consumption ,  Local Information ,  Energy Density ,  Transition Probabilities ,  Energy Distribution ,  Solution Space ,  Root Node ,  Actor Network ,  Reward Function ,  Distributed Manner ,  Observation Space ,  Monte Carlo Tree Search ,  Sink Node ,  Routing Decisions ,  Multi-agent Reinforcement Learning ,  Energy Of Nodes ,  Online Algorithm "
AI Routers & Network Mind: A Hybrid Machine Learning Paradigm for Packet Routing,https://ieeexplore.ieee.org/document/8870277/,35,other,IEEE,2019,"With the increasing complexity of network topologies and architectures, adding intelligence to the network control plane through Artificial Intelligence and Machine Learning (AI&ML) is becoming a trend in network development. For large-scale geo-distributed systems, determining how to appropriately introduce intelligence in networking is the key to high-efficiency operation. In this treatise, we explore two deployment paradigms (centralized vs. distributed) for AI-based networking. To achieve the best results, we propose a hybrid ML paradigm that combines a distributed intelligence, based on units called ""AI routers,"" with a centralized intelligence, called the ""network mind"", to support different network services. In the proposed paradigm, we deploy centralized AI control for connection-oriented tunneling-based routing protocols (such as multiprotocol label switching and segment routing) to guarantee a high QoS, whereas for hop-by-hop IP routing, we shift the intelligent control responsibility to each AI router to ease the overhead imposed by centralized control and use the network mind to improve the global convergence.","Machine Learning ,  Center For Control ,  Control Network ,  Network Services ,  Global Convergence ,  Intelligent Control ,  Control Plane ,  Control Strategy ,  Local Information ,  Graphics Processing Unit ,  Shortest Path ,  Network State ,  Closed-loop Control ,  Multi-agent Systems ,  Deep Reinforcement Learning ,  Markov Decision Process ,  Wireless Sensor Networks ,  Data Packets ,  Reinforcement Learning Algorithm ,  Network Congestion ,  Virtual Network Functions ,  Routing Algorithm ,  Routing Scheme ,  Multi-agent Reinforcement Learning ,  Download Link ,  Traffic Routes ,  Network Routing ,  Traditional Network ,  Responsibility For Control ,  Global Perspective "
MLRS-RL: An Energy-Efficient Multilevel Routing Strategy Based on Reinforcement Learning in Multimodal UWSNs,https://ieeexplore.ieee.org/document/10041116/,2,Journal Article,IEEE,2023,"In recent years, multimodal underwater wireless sensor networks (M-UWSNs) have attracted widespread concern in academia. Due to the complex underwater communication environment and the increasing marine applications, it is a crucial issue for M-UWSNs to design an energy-efficient routing strategy that can satisfy multiple transmission latency requirements of different marine applications. Reinforcement learning (RL) approaches with distributed dynamic optimization ability provide a prospective way to solve the aforementioned problem. Therefore, we propose an improved RL framework and then design an energy-efficient multilevel routing strategy (MLRS-RL) based on this framework for multiple transmission latency requirements. In MLRS-RL, a method of model knowledge collection based on the time backoff principle is proposed to preliminarily learn the network environment information before network operates. The convergence speed of the RL framework can be accelerated by utilizing the model knowledge. Then, underwater nodes use the improved RL model to calculate the transmission rewards that data packets with different transmission latency requirements are sent to different candidate relay nodes. Finally, a cooperative transmission strategy using multiple relay nodes is designed to further improve the reliability of data transmission. We verify the effectiveness of the MLRS-RL strategy in terms of packet delivery ratio, transmission latency, energy efficiency, network lifetime, and delivery quantity.","Routing Scheme ,  Collection Methods ,  Energy Efficiency ,  Data Transmission ,  Application Requirements ,  Network Operators ,  Network Environment ,  Wireless Sensor Networks ,  Data Packets ,  Cooperative Strategy ,  Transmission Scheme ,  Undersea ,  Packet Transmission ,  Reinforcement Learning Model ,  Reinforcement Learning Framework ,  Relay Nodes ,  Multilevel Strategy ,  Transmission Latency ,  Network Lifetime ,  Underwater Communication ,  Autonomous Underwater Vehicles ,  Transmission Patterns ,  Data Packet Transmission ,  Nodes In Set ,  Sink Node ,  Network Energy Consumption ,  Off-line Training ,  Nodal Coordinates ,  Multiple Frequency Bands ,  Energy Of Nodes "
DRED: A DRL-Based Energy-Efficient Data Collection Scheme for UAV-Assisted WSNs,https://ieeexplore.ieee.org/document/10072881/,0,Conference Paper,IEEE,2022,"In Wireless Sensor Networks (WSNs), sensors collect and transmit information to the sink node through single-hop or multi-hop wireless communication links. However, the traditional static sink node solution will cause the hotspot problem due to the energy limitation of sensor nodes. To alleviate the above problem, the Unmanned Aerial Vehicle (UAV)-assisted WSNs, which employs a UAV as the sink node, is proposed to flexibly adjust the routing scheme and prolong the lifetime of sensor nodes. However, the movement of the UAV needs to adapt to the sensor nodes' energy consumption during the transmission in the WSNs, which is a challenging task. Therefore, we propose DRED, an energy-efficient data collection scheme for UAV-assisted WSNs, to control the dynamic routing and the movement of the UAV based on Deep Reinforcement Learning (DRL). The simulation results show that DRED can achieve high network performance in terms of network lifetime.","Wireless Sensor Networks ,  UAV-assisted Wireless Sensor Networks ,  Energy Consumption ,  Unmanned Aerial Vehicles ,  Deep Reinforcement Learning ,  Sink Node ,  Network Lifetime ,  Static Nodes ,  Energy Density ,  Internet Of Things ,  Root Node ,  Central Node ,  Directed Graph ,  Random Strategy ,  Node Positions ,  Reward Function ,  Markov Decision Process ,  Cluster Nodes ,  Movement Trajectories ,  Communication Range ,  Energy Of Nodes ,  Deep Reinforcement Learning Agent ,  Proximal Policy Optimization ,  Static Scheme ,  Residual Energy ,  Normal Nodes ,  Hidden Layer Size ,  Routing Information ,  Learning Rate ,  Data Transmission "
A Reliable and Low-Latency Graph-Routing Approach for IWSN using Q-Routing,https://ieeexplore.ieee.org/document/9277857/,3,Conference Paper,IEEE,2020,"For Industrial Wireless Sensor Networks, the Network Manager is responsible for the overall configuration, route definition, allocation of communication resources, and optimization of the network. Graph routing is used to increase the reliability of the network through path redundancy. Reinforcement Learning models have been used to optimize latency, energy consumption, and data delivery. Q- Routing is a learning model where nodes in the network learn which of its neighboring nodes provide the best routes for a destination node. Despite presenting good results, this model is not applicable to centralized networks since it does not provide path redundancy and nodes are not allowed to choose routes. We present the Q- Learning Reliable Routing with Multiple Agents approach, that builds routing graphs in a centralized manner using Q-Routing. Each node is represented by a learning agent. Periodically, each agent acts by choosing neighbors used to forward data to a destination. An updated graph is then built and configured over the network. Rewards are given to each agent when its average data latency decreases. Simulations were conducted on a WirelessHART simulator. Results show, in most cases, a reduction of the average network latency while the communication reliability is at least as good as the state-of-the-art graph-routing algorithms.","Industrial Wireless Sensor Networks ,  Energy Consumption ,  Optimal Network ,  Wireless Sensor ,  Wireless Sensor Networks ,  Network Management ,  Average Latency ,  Learning Agent ,  Destination Node ,  Performance Metrics ,  Learning Curve ,  Network Topology ,  Wireless Networks ,  Graph Construction ,  Centralized Approach ,  Specific Node ,  Residual Energy ,  Scheduling Algorithm ,  Poor Connectivity ,  Ad Hoc Networks ,  Routing Algorithm ,  Percentage Of Nodes ,  Multi-agent Reinforcement Learning ,  Medium Access Control Layer ,  Battery Lifetime ,  Industrial Internet Of Things ,  1-minute Intervals ,  Long-term Reward ,  Network Reconfiguration ,  Time Interval "
A new Reinforcement Learning based for Energy-efficient Multi-channel Data Gathering in Wireless Sensor Networks,https://ieeexplore.ieee.org/document/9416546/,0,Conference Paper,IEEE,2020,"Wireless sensing networks (WSNs) have attracted widespread attention in the last few years as they have become necessary in recent fields such as the Internet of Things (IoT) which have caused in countless applications. The use of multichannel technology in such kind of networks represents a challenging field due to its advantage in improving throughput and latency, while the major challenge that faces WSNs is the drain of energy. Moreover, adapting to the dynamic property of the transmission flow for channel assignment in an energy-efficient manner is considered as an NP-hard problem. Hence, a Reinforcement Learning (RL) approach is proposed to overcome this challenge. Meanwhile, the use of the RL approach requires a number of iterations to obtain the best solution which in turn creates a considerable communication overhead and collisions which leads to more energy consumption. In this paper, we propose an Energy-efficient method for Reinforcement Learning based Multi-channel MAC (ERL MMAC) that performs a hybrid channel assignment using a decentralized tree for multi-channel data gathering in WSNs. The proposal focuses on the reduction of energy consumption by using the least chosen default channel allocation in two hops rather than one hop in order to reduce as much as possible the conflict links in one side, and use of parent selection strategy rather than parent default channel selection strategy in the learning phase to avoid the redundant data messages in the other side. The results of extensive simulation experiments show the effectiveness of our approach in improving network lifetime with a rate of 97.53%.","Wireless Sensor Networks ,  Collision ,  Energy Consumption ,  Selection Strategy ,  Internet Of Things ,  Learning Phase ,  Reduce Energy Consumption ,  Channel Selection ,  Communication Overhead ,  Reinforcement Learning Approach ,  Network Lifetime ,  Data Messages ,  Learning Process ,  Root Node ,  Neighboring Nodes ,  Successful Action ,  Initiation Phase ,  Leaf Node ,  Routing Information ,  Sink Node ,  Static Strategy ,  Transmission Scheduling "
Q Learning-Based Routing Protocol With Accelerating Convergence for Underwater Wireless Sensor Networks,https://ieeexplore.ieee.org/document/10439015/,0,Journal Article,IEEE,2024,"Underwater wireless sensor networks (UWSNs) have emerged as a promising technology for various underwater applications. Considering the characteristics such as limited energy and high end-to-end delay in UWSNs, it is important to design an underwater routing protocol with high energy efficiency, low end-to-end delay, and high reliability. Therefore, a 
 ${Q}$ 
 learning (QL)-based routing protocol is proposed in this article. First, a 
 ${Q}$ 
 learning-based framework is constructed by considering link connectivity and the information of residual energy, depth, and neighboring nodes. The framework enables protocols to adapt to the dynamic environment and facilitate efficient transmission. Furthermore, to address the slow convergence of 
 ${Q}$ 
 learning in UWSNs, a 
 ${Q}$ 
 value initialization strategy using layer information is designed to accelerate the convergence speed. In addition, an adaptive discount mechanism and a dynamic learning mechanism are proposed to update 
 ${Q}$ 
 values for adapting to the changing network topology and improve the reliability of 
 ${Q}$ 
 values for nodes rarely selected, respectively. Finally, the superior performance of the proposed protocol is evaluated through simulations. Simulation results show that the proposed protocol can still accelerate the convergence speed in reducing the energy tax by 
 ${37}.{16}\%$ 
 and 
 ${23}.{08}\%$ 
, and the average end-to-end delay by 
 ${29}.{94}\%$ 
 and 
 ${16}.{91}\%$ 
 as compared to other 
 ${Q}$ 
 learning-based routing protocols QELAR and QDAR under dynamic environment, while maintaining a higher packet delivery ratio (PDR).","Sensor Networks ,  Underwater Wireless Sensor Networks ,  Energy Efficiency ,  Convergence Rate ,  Network Topology ,  Dynamic Environment ,  Dynamic Mechanism ,  Transmission Efficiency ,  Neighboring Nodes ,  Value Of Node ,  Residual Energy ,  Updated Values ,  Connecting Link ,  Packet Delivery Ratio ,  Energy Consumption ,  Learning Rate ,  Topological Changes ,  Reward Function ,  Optimal Path ,  Markov Decision Process ,  Relay Nodes ,  Energy Of Nodes ,  Node Depth ,  Routing Path ,  Undersea ,  Node Information ,  Control Packets ,  Node Deployment ,  Packet Forwarding ,  Improve Energy Efficiency "
Intelligent Algorithms for the Auto-configuration of Ad Hoc Wireless Networks based on Quality of Service Parameters,https://ieeexplore.ieee.org/document/9590984/,0,Conference Paper,IEEE,2021,"Ad Hoc networks do not depend on infrastructure, this makes each node participating in the routes by forwarding information to the different neighboring nodes and grants autonomy and flexibility to the network. The instability of the wireless network is a problem that affects the Quality of Service (QoS) parameters due to the mobility of the nodes. This article uses an unsupervised learning algorithm and a reinforcement learning algorithm, for the self-configuration of an ad hoc network based on QoS parameters, with a hierarchical network topology that allows its segmentation into clusters, reducing the routing tables. The results show that the use of artificial intelligence algorithms allows the network to remain stable and to improve the conditions around the network management strategy, modifying in realtime the waiting time of the active route and the hello-interval in the AODV protocol. The experiments with the two intelligent algorithms allow analyzing the QoS parameters in each node of the ad hoc wireless network, using the end-to-end delay data of each node, and a dataset of the traffic sent from the entire topology for searching the nodes that require auto-configuration.","Service Quality ,  Wireless Networks ,  Intelligence Algorithms ,  Ad Hoc Networks ,  Quality Of Service Parameters ,  Learning Algorithms ,  Unsupervised Learning ,  Artificial Intelligence Algorithms ,  Reinforcement Learning Algorithm ,  Mobile Nodes ,  Route Time ,  Routing Table ,  Machine Learning ,  Throughput ,  Deep Learning ,  Types Of Networks ,  Intelligent Systems ,  Increase In Performance ,  Sensor Networks ,  Network Throughput ,  Q-learning Algorithm ,  Wireless Sensor Networks ,  K-means Algorithm ,  Simulated Networks ,  Stable Network ,  Anomaly Detection ,  Network Delay ,  Radio Waves ,  Network Efficiency "
Applications of Machine Learning in Networking: A Survey of Current Issues and Future Challenges,https://ieeexplore.ieee.org/document/9388670/,24,Journal Article,IEEE,2021,"Communication networks are expanding rapidly and becoming increasingly complex. As a consequence, the conventional rule-based algorithms or protocols may no longer perform at their best efficiencies in these networks. Machine learning (ML) has recently been applied to solve complex problems in many fields, including finance, health care, and business. ML algorithms can offer computational models that can solve complex communication network problems and consequently improve performance. This paper reviews the recent trends in the application of ML models in communication networks for prediction, intrusion detection, route and path assignment, Quality of Service improvement, and resource management. A review of the recent literature reveals extensive opportunities for researchers to exploit the advantages of ML in solving complex performance issues in a network, especially with the advancement of software-defined networks and 5G.","Machine Learning Applications ,  Learning Algorithms ,  Complex Network ,  Service Quality ,  Resource Management ,  Machine Learning Models ,  Communication Network ,  Advanced Machine Learning ,  Intrusion Detection ,  Trends In Applications ,  Deep Learning ,  Support Vector Machine ,  Artificial Neural Network ,  Decision Tree ,  Wireless Networks ,  Privacy Issues ,  Network Resources ,  Imbalanced Datasets ,  Reinforcement Learning Algorithm ,  5G Networks ,  Intrusion Detection System ,  Quality Of Service Requirements ,  Extreme Learning Machine ,  Bat Algorithm ,  Echo State Network ,  Train Machine Learning Algorithms ,  Brute-force Attacks ,  Routing Scheme ,  Network Slicing ,  High Computational Load "
SDCoR: Software Defined Cognitive Routing for Internet of Vehicles,https://ieeexplore.ieee.org/document/8306876/,51,Journal Article,IEEE,2018,"The Internet of Vehicles (IoV) is a subapplication of the Internet of Things in the automotive field. Large amounts of sensor data require to be transferred in real-time. Most of the routing protocols are specifically targeted to specific situations in IoV. But communication environment of IoV usually changes in the space-time dimension. Unfortunately, the traditional vehicular networks cannot select the optimal routing policy when facing the dynamic environment, due to the lack of abilities of sensing the environment and learning the best strategy. Sensing and learning constitute two key steps of the cognition procedure. Thus, in this paper, we present a software defined cognitive network for IoV (SDCIV), in which reinforcement learning and software defined network technology are considered for IoV to achieve cognitive capability. To the best of our knowledge, this paper is the first one that can give the optimal routing policy adaptively through sensing and learning from the environment of IoV. We perform experiments on a real vehicular dataset to validate the effectiveness and feasibility of the proposed algorithm. Results show that our algorithm achieves better performance than several typical protocols in IoV. We also show the feasibility and effectiveness of our proposed SDCIV.","Internet Of Vehicles ,  Internet Of Things ,  Dynamic Environment ,  Optimal Policy ,  Network Technology ,  Cognitive Networks ,  Vehicular Networks ,  Learning Algorithms ,  Dynamic Network ,  Average Speed ,  Base Station ,  Traffic Flow ,  Mobile Network ,  Learning Module ,  Reward Function ,  Network Environment ,  Simulated Networks ,  Control Logic ,  Ad Hoc Networks ,  Routing Algorithm ,  Vehicular Ad Hoc Networks ,  Delivery Ratio ,  Roadside Units ,  Vehicular Communication ,  Vehicle Density ,  Control Plane ,  Traffic Simulation ,  Average Vehicle Speed ,  Routing Information ,  Learning Problem "
K-means ++ and Reinforcement Learning-Based Clustering Method to Achieve Energy Efficiency in Underwater Acoustic Sensor Networks,https://ieeexplore.ieee.org/document/10491829/,0,Conference Paper,IEEE,2023,"In recent years, underwater acoustic sensor networks (UASNs) have had a wide range of applications in the fields of underwater environment sensing, marine resource exploitation and water quality analysis. Sensor nodes operate in an underwater environment and are powered by embedded batteries, making it very difficult to replenish power. It is thus important to efficiently improve sensor energy efficiency in UASNs and to extend the network lifetime. Clustering can effectively reduce UASN energy consumption by dividing sensor nodes into clusters; however, energy efficiency is always affected by the imbalance in energy consumption due to the unreasonable selection of cluster heads (CHs) and routes. In this paper, a K-means ++ and reinforcement learning-based clustering (KRC) method is proposed to improve energy efficiency. First, the UASN clusters are uniformly distributed using the K-means ++ algorithm. Then, a combination of clustering and reinforcement learning is applied to CH selection, which calculates the energy of intercluster data transmission when each node in the cluster is a CH based on the node’s residual energy and location and selects the appropriate CHs to equalize the energy consumption of each network node. The optimal transmission route constructed by reinforcement learning is used to select suitable nodes for relaying data for sensor nodes. The simulation results show that the KRC method effectively balances UASN node energy consumption and improves energy efficiency, thereby extending network survival.","Energy Efficiency ,  Sensor Networks ,  Acoustic Sensors ,  Acoustic Networks ,  Energy Consumption ,  Uniform Distribution ,  Wide Range Of Applications ,  Data Transmission ,  Cluster Nodes ,  Residual Energy ,  Undersea ,  Water Quality Analysis ,  Cluster Head ,  Network Lifetime ,  Energy Of Nodes ,  Use Of Marine Resources ,  Learning Algorithms ,  Average Energy ,  Cluster Centers ,  Neighboring Nodes ,  Head Node ,  Sink Node ,  Q-learning Algorithm ,  Wireless Sensor Networks ,  Reward Function ,  Routing Path ,  Round Of Simulations ,  Transmission Process ,  Energy Consumption Model ,  Node Positions "
Reinforcement Learning Based Routing Protocol for Wireless Body Sensor Networks,https://ieeexplore.ieee.org/document/8315358/,16,Conference Paper,IEEE,2017,"Patients must be continuous and consistent way links to their doctors to control continuous health status. Wireless Body Sensor Network (WBSN) plays an important role in communicating the patient's vital information to any remote healthcare center. These networks consist of individual nodes to collect the patient's physiological parameters and communicate with the destination if the sensed parameter value is beyond normal range. Therefore, they can monitor patient's health continuously. The nodes deployed with the patient form a WBSN and so the network send data from source node to the remote sink or base station by efficient links. It is necessary to extend the life of the system by selecting optimized paths. This paper presents a cluster-based routing protocol by new Q-learning approach (QL-CLUSTER) to find best routes between individual nodes and remote healthcare station. Simulations are made with a set of mobile biomedical wireless sensor nodes with an area of 1000 meters x 1000 meters flat space operating for 600 seconds of simulation time. Results show that the QL-CLUSTER based approach requires less time to route the packet from the source node to the destination remote station compared with other algorithms.","Wireless Networks ,  Sensor Networks ,  Wireless Sensor ,  Wireless Sensor Networks ,  Body Area Networks ,  Base Station ,  Mobile Sensors ,  Mobile Nodes ,  Remote Center ,  Biomedical Sensors ,  Random Model ,  Time Duration ,  Energy Efficiency ,  Less Than Or Equal ,  Power Consumption ,  Wearable Devices ,  Power Requirements ,  Data Packets ,  Efficient Route ,  Mobility Model ,  Cluster Head ,  Q-learning Algorithm ,  Destination Node ,  Current Node ,  Multi-hop Networks ,  Receiver Node ,  Packet Delivery Ratio ,  Packet Transmission ,  Energy Of Nodes ,  Range Of Nodes "
A Reinforcement-Learning-Based Opportunistic Routing Protocol for Energy-Efficient and Void-Avoided UASNs,https://ieeexplore.ieee.org/document/9777731/,25,Journal Article,IEEE,2022,"Underwater acoustic sensor networks (UASNs) have been used for various scenarios, such as marine exploration, and development of marine resources. However, due to the challenging underwater environment, UASNs suffers from high propagation delay, low transmission reliability and energy limitation, which pose significant obstacles to the delivery efficiency of data packets. What’s more, the sparse topology, the failure of node or link, and other factors can cause void holes, resulting in the considerable package retransmission and the low network reliability of UASNs. To this end, this paper proposed a reinforcement-learning-based opportunistic routing protocol (ROEVA) to reduce energy consumption as well as to improve the transmission reliability which also addresses the issue of void routing in underwater acoustic sensor networks. To seek optimal routing rules, a reward function based on reinforcement learning is proposed, where factors such as energy, delay, link quality, and depth information are all taken into account for appropriate routing decisions. Before forwarding the data packet, a two-hop availability checking function is defined, which can identify trap nodes and avoid routing holes. In addition, aiming to reduce packet redundancy and collisions, a waiting mechanism derived from opportunistic routing is proposed. According to the calculated Q-values, the waiting mechanism determines the priority list for packet forwarding. Evaluation results demonstrate that the proposed ROEVA protocol outperforms HHVBF, RCAR, QELAR and GEDAR in terms of energy efficiency, packet delivery ratio (PDR), average hop count, and end-to-end delay, which are analyzed by varying the number of nodes from 100 to 500.","Opportunistic Routing ,  Opportunistic Routing Protocol ,  Energy Consumption ,  Energy Efficiency ,  Sensor Networks ,  Average Count ,  Pathfinding ,  Depth Information ,  Reward Function ,  Data Packets ,  Undersea ,  Reliable Transmission ,  Packet Delivery ,  Delivery Ratio ,  Routing Decisions ,  Packet Forwarding ,  Packet Delivery Ratio ,  Hop Count ,  Shortest Path ,  Node Status ,  Sink Node ,  Packet Transmission ,  Residual Energy ,  Neighboring Nodes ,  Packet Header ,  Source Node ,  Optimal Path ,  Routing Path ,  Void Area ,  Acoustic Channel "
Optimizing the Lifetime of Software Defined Wireless Sensor Network via Reinforcement Learning,https://ieeexplore.ieee.org/document/9302576/,22,Journal Article,IEEE,2021,"Reinforcement learning (RL) is an unsupervised learning technique used in many real-time applications. The essence of RL is a decision-making problem. In RL, the agent constantly interacts with the environment and selects the next action according to previous feedback in terms of reward. In this paper, RL trains Software-Defined Wireless Sensor Networks (SDWSNs) controller to optimize the routing paths. We combine RL and SDN, where RL is applied to the SDN controller to generate the routing tables. We also propose four different reward functions for optimization of the network performance. RL-based SDWSN improves network performance by 23% to 30% in terms of lifetime compared with RL-based routing techniques. RL-based SDWSN performs well because it can intelligently learn the routing path at the controller. In addition, it has a faster network convergence rate than RL-based WSN.","Wireless Sensor Networks ,  Convergence Rate ,  Network Performance ,  Reward Function ,  Routing Path ,  Improve Network Performance ,  Routing Table ,  Energy Consumption ,  Graphical Representation ,  Neighboring Nodes ,  Pathfinding ,  Bitrate ,  Deep Reinforcement Learning ,  Markov Decision Process ,  Traffic Control ,  Data Packets ,  Adaptive Selection ,  End Of Round ,  Spanning Tree ,  Routing Algorithm ,  Control Packets ,  Network Lifetime ,  Control Plane ,  Idle Period ,  Traffic Engineering ,  Form Of Reward ,  Energy Consumption Model ,  Load Balancing ,  Path Planning ,  Processing Energy Consumption "
Anypath Routing Protocol Design via Q-Learning for Underwater Sensor Networks,https://ieeexplore.ieee.org/document/9284516/,41,Journal Article,IEEE,2021,"As a promising technology in the Internet of Underwater Things, underwater sensor networks (UWSNs) have drawn a widespread attention from both academia and industry. However, designing a routing protocol for UWSNs is a great challenge due to high energy consumption and large latency in the underwater environment. This article proposes a Q-learning -based localization-free anypath routing (QLFR) protocol to prolong the lifetime as well as reduce the end-to-end delay for UWSNs. Aiming at optimal routing policies, the Q-value is calculated by jointly considering the residual energy and depth information of sensor nodes throughout the routing process. More specifically, we define two reward functions (i.e., depth-related and energy-related rewards) for Q-learning with the objective of reducing latency and extending network lifetime. In addition, a new holding time mechanism for packet forwarding is designed according to the priority of forwarding candidate nodes. Furthermore, mathematical analyses are presented to analyze the performance and computational complexity of the proposed routing protocol. Extensive simulation results demonstrate the superiority performance of the proposed routing protocol in terms of the end-to-end delay and the network lifetime.","Sensor Networks ,  Underwater Sensor ,  Underwater Sensor Networks ,  Routing Protocol Design ,  Energy Consumption ,  Depth Information ,  Reward Function ,  Processing Routes ,  Node Information ,  Residual Energy ,  Undersea ,  Hold Time ,  Network Lifetime ,  Energy Of Nodes ,  Large Latency ,  Packet Forwarding ,  Energy Efficiency ,  Shortest Path ,  Neighboring Nodes ,  Total Energy Consumption ,  Data Packets ,  Packet Transmission ,  Probability Of Delivery ,  Sink Node ,  Routing Path ,  Hop Count ,  Routing Decisions ,  Packet Header ,  Delivery Ratio ,  Node Depth "
Evaluating and Boosting Reinforcement Learning for Intra-Domain Routing,https://ieeexplore.ieee.org/document/9077370/,14,Conference Paper,IEEE,2019,"The success of machine learning in domains such as computer vision and computer games has triggered a surge of interest in applying machine learning in computer networks. This paper tries to answer a broadly-debated question: can we improve the performance of intradomain routing, one of the most fundamental blocks in the Internet, with reinforcement learning (RL)? Due to the complex network traffic conditions and the large action space in routing, it is difficult to give a definite answer for existing RL-based routing solutions. To gain an in-depth understanding on the challenges of RL-based routing, we systematically classify different RL-based routing solutions and investigate the performance of several representative approaches, in terms of scalability, stability, robustness, and convergence. With the lessons learned in evaluating various RL-based routing solutions, we propose two methods, called supervised Q-network routing (SQR) and discrete link weight-based routing (DLWR), which boost the performance of RL-based routing and outperform the de facto shortest path intradomain routing.","ordsIntradomain routing ,  reinforcement learning ,  RL-based routing "
CARMA: Channel-Aware Reinforcement Learning-Based Multi-Path Adaptive Routing for Underwater Wireless Sensor Networks,https://ieeexplore.ieee.org/document/8822792/,78,Journal Article,IEEE,2019,"Routing solutions for multi-hop underwater wireless sensor networks suffer significant performance degradation as they fail to adapt to the overwhelming dynamics of underwater environments. To respond to this challenge, we propose a new data forwarding scheme where relay selection swiftly adapts to the varying conditions of the underwater channel. Our protocol, termed CARMA for Channel-aware Reinforcement learning-based Multi-path Adaptive routing, adaptively switches between single-path and multi-path routing guided by a distributed reinforcement learning framework that jointly optimizes route-long energy consumption and packet delivery ratio. We compare the performance of CARMA with that of three other routing solutions, namely, CARP, QELAR and EFlood, through SUNSET-based simulations and experiments at sea. Our results show that CARMA obtains a packet delivery ratio that is up to 40% higher than that of all other protocols. CARMA also delivers packets significantly faster than CARP, QELAR and EFlood, while keeping network energy consumption at bay.","Wireless Sensor Networks ,  Underwater Wireless ,  Underwater Wireless Sensor Networks ,  Energy Consumption ,  Network Energy ,  Reinforcement Learning Framework ,  Packet Delivery ,  Network Energy Consumption ,  Packet Delivery Ratio ,  Relay Selection ,  Value Function ,  Cost Function ,  Transition Probabilities ,  Network Size ,  Transmission Power ,  Channel State ,  Node Status ,  Neighboring Nodes ,  Bitrate ,  Data Packets ,  Relay Nodes ,  Packet Transmission ,  Control Packets ,  Packet Forwarding ,  Minimum Energy Consumption ,  Residual Energy ,  Packet Header ,  Successful Transmission ,  Data Throughput ,  Data Delivery "
