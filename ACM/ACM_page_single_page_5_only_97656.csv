title,link,number_of_citation,article_type,publisher,publication_date,abstract,keyword
Handling Coexistence of LoRa with Other Networks through Embedded Reinforcement Learning,https://dl.acm.org/doi/10.1145/3576842.3582383,2,research-article,ACM,2023,"The rapid growth of various Low-Power Wide-Area Network (LPWAN) technologies in the limited spectrum brings forth the challenge of their coexistence. Today, LPWANs are not equipped to handle this impending challenge. It is difficult to employ sophisticated media access control protocol for low-power nodes. Coexistence handling for WiFi or traditional short-range wireless network will not work for LPWANs. Due to long range, their nodes can be subject to an unprecedented number of hidden nodes, requiring highly energy-efficient techniques to handle such coexistence. In this paper, we address the coexistence problem for LoRa, a leading LPWAN technology. To improve the performance of a LoRa network under coexistence with many independent networks, we propose the design of a novel embedded learning agent based on a lightweight reinforcement learning at LoRa nodes. This is done by developing a Q-learning framework while ensuring minimal memory and computation overhead at LoRa nodes. The framework exploits transmission acknowledgments as feedback from the network based on what a node makes transmission decisions. To our knowledge, this is the first Q-learning approach for handling coexistence of low-power networks. Considering various coexistence scenarios of a LoRa network, we evaluate our approach through experiments indoors and outdoors. The outdoor results show that our Q-learning approach on average achieves an improvement of 46% in packet reception rate while reducing energy consumption by 66% in a LoRa network. In indoor experiments, we have observed some coexistence scenarios where a current LoRa network loses all the packets while our approach enables 99% packet reception rate with up to 90% improvement in energy consumption.",none
Reinforcement learning based reliability-aware routing in IoT networks,https://dl.acm.org/doi/10.1016/j.adhoc.2022.102869,0,research-article,ACM,2022,"The unprecedented scale and ubiquity of the Internet of Things (IoT) introduce a maintainability challenge. IoT networks operate in diverse and harsh environments that impose thermal stress on IoT devices. The lifetime of these networks can be limited by hardware failures resulting from exacerbated reliability degradation mechanisms at high temperatures. In this paper, we propose a novel adaptive and distributed reliability-aware routing protocol based on reinforcement learning to mitigate the reliability degradation of IoT devices and improve the network Mean Time to Failure (MTTF). Through routing, we curb the utilization of quickly degrading devices, which helps to lower the device power dissipation and temperature, thus reducing the effect of temperature-driven failure mechanisms. To quantify and optimize networking performance besides reliability, we incorporate Expected Transmission Count (ETX) in our formulations as a measure of communication link quality. Our proposed algorithm adapts routing decisions based on the current reliability status of the devices, the amount of degradation they are likely to experience due to communication activity, and network performance goals. We extend the ns-3 network simulator to support our reliability models and evaluate the routing performance by comparing with state-of-the-art approaches. Our results show up to a 73.2% improvement in reliability for various communication data rates and the number of nodes in the network while delivering comparable performance.",none
Dynamic link utilization empowered by reinforcement learning for adaptive storage allocation in MANET,https://dl.acm.org/doi/10.1007/s00500-023-09281-8,0,research-article,ACM,2023,"In modern wireless networks, mobile nodes often deal with the challenge of maintaining a sufficient number of data packets due to limited storage capacity within each cluster. It adversely impacts network performance by compromising data quality during transmissions. The ensuing delays, caused by data packets awaiting storage allocation, result in reduced throughput and increased end-to-end latency. To effectively address these issues, we present a Dynamic Link Utilization with Reinforcement Learning (DLU-RL) method, which is designed to optimize storage allocation for communication data packets, significantly enhancing network performance. Instead of static allocation, DLU-RL employs dynamic strategies guided by reinforcement learning algorithms. This innovative method not only tackles storage constraints but also proactively adapts to varying network conditions and traffic patterns. In our approach, we first perform a comprehensive analysis of storage capacities across all nodes, establishing a baseline for dynamic resource allocation. The DLU-RL framework then swiftly assigns storage space based on real-time demand and priority, optimizing storage utilization on the fly. As a result of implementing DLU-RL, substantial enhancements in throughput and concurrent minimization of end-to-end delays are achieved. This research not only contributes to efficient storage allocation techniques but also pioneers the integration of reinforcement learning for wireless communication network performance optimization. The proposed framework signifies a paradigm shift in storage management, offering adaptability, efficiency, and real-time optimization to tackle the evolving challenges of wireless communication.",none
High performance wireless sensor network localisation system,https://dl.acm.org/doi/10.1504/IJAHUC.2014.065776,4,article,ACM,2014,"In this paper we summarise the results of our research concerned with the development, implementation and evaluation of a software framework for wireless sensor networks (WSNs) localisation - high performance localisation system (HPLS). The system can be used to calculate positions of sensing devices (network nodes) in the deployment area, and to tune and verify various localisation schemes through simulation. It provides tools for data acquisition from a workspace, estimation of inter-node distances, calculation of geographical coordinates of all nodes with unknown position and results evaluation. Received Signal Strength measurements are utilised to support the localisation process. Trilateration, simulated annealing (SA) and genetic algorithm (GA) are applied to calculate the geographical coordinates of network nodes. The utility, efficiency and scalability of the proposed localisation system HPLS have been justified through simulation and testbed implementation. The calculations have been done in parallel using the map-reduce paradigm and the high performance computing (HPC) environment formed by a cluster of servers. The testbed networks were formed by sensor devices manufactured by Advantic Technology (clones of TelosB platform). A provided case study demonstrates the localisation accuracy obtained for small-, medium and large-size multihop networks.",none
Multi-user reinforcement learning based task migration in mobile edge computing,https://dl.acm.org/doi/10.1007/s11704-023-1346-3,1,research-article,ACM,2023,"Mobile Edge Computing (MEC) is a promising approach. Dynamic service migration is a key technology in MEC. In order to maintain the continuity of services in a dynamic environment, mobile users need to migrate tasks between multiple servers in real time. Due to the uncertainty of movement, frequent migration will increase delays and costs and non-migration will lead to service interruption. Therefore, it is very challenging to design an optimal migration strategy. In this paper, we investigate the multi-user task migration problem in a dynamic environment and minimizes the average service delay while meeting the migration cost. In order to optimize the service delay and migration cost, we propose an adaptive weight deep deterministic policy gradient (AWDDPG) algorithm. And distributed execution and centralized training are adopted to solve the high-dimensional problem. Experiments show that the proposed algorithm can greatly reduce the migration cost and service delay compared with the other related algorithms.",none
Deep reinforcement learning-based edge computing offloading algorithm for software-defined IoT,https://dl.acm.org/doi/10.1016/j.comnet.2023.110006,0,research-article,ACM,2023,"Edge computing offloading can effectively solve the problem of insufficient computing resources for terminal devices and improve the performance and efficiency of the system. When network states and tasks change rapidly, data-driven intelligent algorithms have difficulty obtaining comprehensive statistics for accurate prediction, resulting in degraded performance of computational offloading and difficulty in adaptive adjustment. It is a current challenge to improve the environment-aware, intelligent optimization so that the computational offloading algorithm can adapt to the dynamic changes in network state and task demands, thus achieving global multi-objective optimization. This paper presents optimized edge computing offloading algorithm for software-defined IoT. First, to provide global state for making decisions, a software defined edge computing (SDEC) architecture is proposed. The edge layer is integrated into the control layer of software-defined IoT, and multiple controllers share the global network state information via eastâ€“west message exchange. Moreover, an edge computing offloading algorithm in software-defined IoT (ECO-SDIoT) based on deep reinforcement learning is proposed. It enables the controllers to offload the computing task to the most appropriate edge server according to the global states, task requirements, and reward. Finally, the performance metrics for edge computing offloading were evaluated in terms of unit task processing latency, load balancing of edge servers, task processing energy consumption, and task completion rate, respectively. Simulation results show that ECO-SDIoT can effectively reduce task completion time and energy consumption compared with other strategies.Highlightsâ€¢We proposed SDEC framework in IoT environment to provide support for DRL.â€¢We proposed ECO-SDIoT algorithm to derive the optimal offloading policy.â€¢We create appropriate rewards to feedback information about computing offloading.â€¢Simulation results show that ECO-SDIoT algorithm outperforms the three related works.",none
Reinforcement learning-based multi-objective energy-efficient task scheduling in fog-cloud industrial IoT-based systems,https://dl.acm.org/doi/10.1007/s00500-023-09159-9,0,research-article,ACM,2023,"The advancement of Industrial Internet of Things (IIoT) applications has increased the demand for efficient and energy-aware task scheduling in fog-cloud environments. This paper presents a novel multi-objective energy efficient task scheduling based reinforcement learning (MEETS-RL) model for fog-cloud IIoT-based systems. The IIoT, fog, and cloud are the three layers obtained in the proposed method. Tasks are captured from various industries through industrial devices such as actuators, sensors, and control systems placed near fog nodes. The task classification stage utilizes the ID3 algorithm to classify tasks based on their priority, Quality of Service (QoS), and processing requirements. After classification, appropriate fog nodes are selected for task execution considering factors such as energy consumption, processing capabilities, and proximity to IIoT devices. The unhandled task in fog nodes is offloaded to cloud data centers using the first fit (FF) algorithm, based on available resources, QoS requirements, and fog node location. Once the suitable fog nodes or cloud data centers have been identified, the tasks are scheduled for execution using a reinforcement learning-based task scheduling algorithm. Experimental evaluations demonstrate that the proposed MEETS-RL model outperforms existing task scheduling models, including first come first serve (FCFS), greedy for energy (GfE), shortest job first (SJF), round Robin (RR), and Laxity based priority with Ant colony system (LBP-ACS). The experimental results of the proposed architecture showed an accuracy rate of 98.8%, task completion rate of 95% and energy consumption of 0.4Â J. The proposed architecture offers a comprehensive solution for modern IIoT systems demanding flexibility, scalability, and efficient resource management while enhancing energy efficiency through the application of reinforcement learning in task scheduling.",none
Recent Reinforcement Learning and Blockchain Based Security Solutions for Internet of Things: Survey,https://dl.acm.org/doi/10.1007/s11277-023-10664-1,1,research-article,ACM,2023,"Usersâ€™ security is one of the most important issues in Internet of Things (IoT) due to the high number of IoT devices involved in different applications. Security threats are evolving at a rapid pace that make the current security and privacy measures unsuitable. Therefore, several researchers have been attracted by this domain with the aim of proposing either new or improved solutions to address the problem of security in IoT. Blockchain technology is a relatively new invention in modern IoT applications to solve the security issue. It is based on the use of a public immutable ledger called a blockchain. After conducting a verification process, several parts on a network encode transactions into this ledger. Moreover, Machine learning (ML) algorithms have been used as emerging solutions to improve IoT security. Reinforcement learning (RL) is the most popular machine learning technique proposed to secure IoT systems. Unlike other ML methods, RL can observe, learn and interact with the environment even if it has minimum information about the considered parameters. Various researches have been proposed to treat security problem in IoT based on either RL technique or Blockchain technology or a combination of both techniques. Therefore, we believe there is a need for a comprehensive survey on works proposed in recent years that address security issues using these techniques. In this paper, we provide a summary of research efforts made in the past few years, from 2018 to 2021, addressing security issues using RL and blockchain techniques in the IoT domain.",none
Node position estimation based on optimal clustering and detection of coverage hole in wireless sensor networks using hybrid deep reinforcement learning,https://dl.acm.org/doi/10.1007/s11227-023-05494-8,0,research-article,ACM,2023,"Sensor nodes, typically small and low-power devices, are components of wireless sensor networks (WSNs). Each node monitors its surroundings for relevant environmental changes and sends all detected events to the data collector for analysis. If the sensor nodes are not placed correctly, there may be areas that are not within the detection zone of any sensor node. Coverage holes in WSNs are usually caused by random deployment and node failure. Energy holes and dead nodes are the main problems caused by detection and recovery of coverage holes in WSNs. The size of coverage holes increases the time complexity and power of recent protocols. However, there is a high computational complexity associated with distributed methods proposed in recent years to solve the coverage hole detection problem. In this paper, we propose optimal cluster-based node position estimation and coverage hole detection in WSNs using a hybrid deep learning approach. First, a modified Lyapunov optimization (MLO) algorithm to compute the node position is presented, which ensures edge nodes in the network. Next, we design optimal clustering technique by using improved sand cat swarm optimization (ISCSO) algorithm to formulate efficient balanced clusters which computes coverage hole area in the network. Afterward, we develop a hybrid deep reinforcement learning (Hyb-DRL) technique for hole shape detection and hole size judgment within clusters, among clusters and along edges. The results show that the proposed approach achieves significant improvements compared to existing benchmark approaches. Specifically, the average energy consumption of CG-DCHD approach is 43.835%, 32.674% and 26.164% lower for node density, hole density and simulation rounds, respectively. The hole detection time is 18.4%, 16.802% and 15.462% lower, while the coverage is 16.885%, 14.977% and 12.219% higher for node density, hole density and simulation rounds, respectively. Additionally, the network lifetime of CG-DCHD approach is 15.58%, 17.702% and 20.492% higher, while the control packet overhead is 0.83%, 1.907% and 1.466% lower for node density, hole density and simulation rounds, respectively.",none
"Route Packets Uneven, Consume Energy Even: A Lifetime Expanding Routing Algorithm for WSNs",https://dl.acm.org/doi/10.1007/s11277-022-09702-1,0,research-article,ACM,2022,"Wireless Sensor Networks (WSNs) are infrastructure-free networks consisting of tiny and simple environmental sensing devices. Sensor nodes collaborate through wireless and limited-range communication links to report environmental conditions to the network base-station (BS). These features need to establish efficient routing algorithms in WSNs to optimize various parameters of WSNs, including energy consumption, end-to-end delay, network lifetime, and network congestion. This paper proposes an efficient routing algorithm to evenly consume energy over the network area that eventually expands the network lifetime. The proposed algorithm directs an event occurrence report toward the BS in a multi-hop manner. Intermediate sensor nodes decide about the next receiving node of the report using a lightweight optimization process based on the networkâ€™s mean residual energy, sensorsâ€™ distance to BS, and the positions of neighboring sensors. Evaluations show that the proposed algorithm directs traffic toward the boundaries of the network, which leads to better balance in traffic and energy consumption network-wide. The proposed algorithm reduces the standard deviation of the sensor node residual energy up to 6x and, through this, expands the network lifetime by at least 36%.",none
Novel metaheuristic routing algorithm with optimized energy and enhanced coverage for WSNs,https://dl.acm.org/doi/10.1016/j.adhoc.2023.103133,1,research-article,ACM,2023,"tsâ€¢This research will be helpful for researchers who want to use wireless sensor networks with increased network coverage in their applications.â€¢Enhances network coverage with optimized energy.â€¢Achieve maximum coverage but no increase in sensor nodes means no increase in the cost of equipment.â€¢Provide comparative analysis with other existing algorithms.AbstractThe distribution of sensor nodes is the key factor in mobile wireless sensor networks (WSNs) to enhance the functionality of the network. The effectiveness of a wireless sensor network can be increased by the efficient distribution of several sensor nodes. Previously researchers considered existing algorithms or their hybrid for the effective distribution of sensor nodes. Those only optimize energy or enhance network coverage none of them considered both aspects with the minimum cost of equipment. This piece of work proposes a novel bio-inspired ruminant algorithm. The algorithm is developed as a result of the in- spiration of the efficient digestive system of ruminant animals. That takes a large amount of raw food as input and produces an optimal value of food that is full of energy. The proposed algorithm is validated on several benchmark functions. By keeping inspiration in mind, the proposed algorithm not only applied to enhance network coverage of WSNs with optimized energy and sensor nodes distribution, which increases the lifetime of the equipment. Furthermore, the results of the algorithm show that if network coverage enhances more optimized value of energy is achieved with no increase in the number of deployed sensors. That proves the no increase in the cost of the equipment with more network coverage on enhanced lifetime.",none
Reinforcement learning based energy-neutral operation for hybrid EH powered TBAN,https://dl.acm.org/doi/10.1016/j.future.2022.10.037,0,research-article,ACM,2023,"The aging population, outbreak of new infectious diseases and shortage of medical resources promote rapid development of telemedicine. Wireless textile body area network (TBAN), which combines functional textile and wireless body area network (WBAN), is gaining great attention as an efficient medium of remote medical care. This is because of its unique materials and application scenario, as well as its convenience and friendliness to the elderly. Moreover, it is an effective application for integrating edge computing with next generation of wearable technology. Nonetheless, it is unavoidable that TBAN has to deal with reliability and energy issues. Given these deficiencies and challenges, this paper focuses on the feasibility of achieving wearable energy neutral operation (ENO) in TBAN while maintaining robustness. In addition to adding user posture factors regarding network specifics, we combine hybrid energy harvesting (EH) techniques and duty cycle schemes. A hybrid radio frequency (RF) energy and Triboelectric nanogenerator (TENG) EH-assisted TBAN system is built in this work. We analyze and discuss the delay, data rate and packet error rate (PER) under five typical daily activities (standing, sitting, lying, walking, and running). To optimize the ENO problem, two reinforcement learning (Q-learning and Deep Q-Network (DQN)) based algorithms are proposed. According to numerical results, both algorithms ultimately lead to stable power levels compared to the continuous decline of battery power without optimization. DQN-based optimization performs better than Q-Learning. For instance, 14% and 56% improvements in PER and battery power, respectively.Highlightsâ€¢A hybrid EH along with rechargeable battery powered TBAN model is built and proposed.â€¢Energy efficiency & network reliability are analyzed with different daily activities.â€¢The feasibility of ENO is achieved and verified by Q-Learning and DQN based methods.",none
Reinforcement and deep reinforcement learning for wireless Internet of Things: A survey,https://dl.acm.org/doi/10.1016/j.comcom.2021.07.014,5,review-article,ACM,2021,"Nowadays, many research studies and industrial investigations have allowed the integration of the Internet of Things (IoT) in current and future networking applications by deploying a diversity of wireless-enabled devices ranging from smartphones, wearables, to sensors, drones, and connected vehicles. The growing number of IoT devices, the increasing complexity of IoT systems, and the large volume of generated data have made the monitoring and management of these networks extremely difficult. Numerous research papers have applied Reinforcement Learning (RL) and Deep Reinforcement Learning (DRL) techniques to overcome these difficulties by building IoT systems with effective and dynamic decision-making mechanisms, dealing with incomplete information related to their environments. The paper first reviews pre-existing surveys covering the application of RL and DRL techniques in IoT communication technologies and networking. The paper then analyzes the research papers that apply these techniques in wireless IoT to resolve issues related to routing, scheduling, resource allocation, dynamic spectrum access, energy, mobility, and caching. Finally, a discussion of the proposed approaches and their limits is followed by the identification of open issues to establish grounds for future research directions proposal.",none
EQRSRL: an energy-aware and QoS-based routing schema using reinforcement learning in IoMT,https://dl.acm.org/doi/10.1007/s11276-023-03367-9,0,research-article,ACM,2023,"Internet of Medical Thing (IoMT) is an emerging technology in healthcare that can be used to realize a wide variety of medical applications. It improves peopleâ€™s quality of life and makes it easier to care for the sick individuals in an efficient and safe manner. To do this, IoMT leverages the capabilities of some new technologies including IoT, Artificial Intelligence, cloud computing, computer networks and medicine. Combining these technologies to monitor the patientâ€™s health conditions in real-time or semi-real-time is a critical challenge in IoMT. In this regard, one of the most crucial components of IoMT are network communication protocols that should provide a fast and reliable communication path between a connected biosensor to a patient and cloud computing environment. In this paper, we propose EQRSRL as an efficient routing mechanism for different types of IoMT applications. The aim of EQRSRL is to provide a reasonable level of Quality of Service (QoS) for IoMT traffics. To achieve this goal, it categorizes the network traffic into three classes and treats them differently concerning their QoS requirements. Moreover, EQRSRL divides the network environment into multiple zones to decrease the number of message exchange between the nodes. In order to compute optimal paths between the nodes, it considers QoS and energy metrics, and makes use of a reinforcement learning approach in path computation process. Simulation results show that the implementation of EQRSRL in IoMT is practical and leads to improvement of 82% in average energy consumption, 25% in end-to-end delay and 7% packet delivery ration in compared to the state-of-the-art routing techniques.",none
Reinforcement Learning-Based MAC Protocol for Underwater Multimedia Sensor Networks,https://dl.acm.org/doi/10.1145/3484201,6,research-article,ACM,2022,"High propagation delay, high error probability, floating node mobility, and low data rates are the key challenges for Underwater Wireless Multimedia Sensor Networks (UMWSNs). In this article, we propose RL-MAC, a Reinforcement Learning (RL)â€“based Medium Access Control (MAC) protocol for multimedia sensing in an Underwater Acoustic Network (UAN) environment. The proposed scheme uses Transmission Opportunity (TXOP) for relay nodes in a multi-hop network for improved efficiency concerning the mobility of the relays and sensor nodes. The access point (AP) and relay nodes calculate traffic demands from the initial contention of the sensor nodes. Our solution uses Q-learning to enhance the contention mechanism at the initial phase of multimedia transmission. Based on the traffic demands, RL-MAC allocates TXOP duration for the uplink multimedia reception. Further, the Structural Similarity Index Measure (SSIM) and compression techniques are used for calculating the image quality at the receiver end and reducing the image at the destination, respectively. We implement a prototype of the proposed scheme over an off-the-shelf, low-cost hardware setup. Moreover, extensive simulation over NS-3 shows a significant packet delivery ratio and throughput compared with the existing state-of-the-art.",none
A reinforcement learning-based sleep scheduling algorithm for compressive data gathering in wireless sensor networks,https://dl.acm.org/doi/10.1186/s13638-023-02237-4,2,research-article,ACM,2023,"Compressive data gathering (CDG) is an adequate method to reduce the amount of data transmission, thereby decreasing energy expenditure for wireless sensor networks (WSNs). Sleep scheduling integrated with CDG can further promote energy efficiency. Most of existing sleep scheduling methods for CDG were formulated as centralized optimization problems which introduced many extra control message exchanges. Meanwhile, a few distributed methods usually adopted stochastic decision which could not adapt to variance in residual energy of nodes. A part of nodes were prone to prematurely run out of energy. In this paper, a reinforcement learning-based sleep scheduling algorithm for CDG (RLSSA-CDG) is proposed. Active nodes selection is modeled as a finite Markov decision process. The mode-free Q learning algorithm is used to search optimal decision strategies. Residual energy of nodes and sampling uniformity are considered into the reward function of the Q learning algorithm for load balance of energy consumption and accurate data reconstruction. It is a distributed algorithm that avoids large amounts of control message exchanges. Each node takes part in one step of the decision process. Thus, computation overhead for sensor nodes is affordable. Simulation experiments are carried out on the MATLAB platform to validate the effectiveness of the proposed RLSSA-CDG against the distributed random sleep scheduling algorithm for CDG (DSSA-CDG) and the original sparse-CDG algorithm without sleep scheduling. The simulation results indicate that the proposed RLSSA-CDG outperforms the two contrast algorithms in terms of energy consumption, network lifetime, and data recovery accuracy. The proposed RLSSA-CDG reduces energy consumption by 4.64% and 42.42%, respectively, compared to the DSSA-CDG and the original sparse-CDG, prolongs life span by 57.3%, and promotes data recovery accuracy by 84.7% compared to the DSSA-CDG.",none
A hybrid model using fuzzy logic and an extreme learning machine with vector particle swarm optimization for wireless sensor network localization,https://dl.acm.org/doi/10.1016/j.asoc.2018.01.004,23,research-article,ACM,2018,"l abstractDisplay OmittedHighlightsâ€¢We investigate a practical integration of soft-computing for WSN localizations.â€¢We propose a hybrid model applying Centroid and ELM optimizations.â€¢We optimize the hybrid model using resultant vector based on PSO.â€¢We consider several key factors affecting the location estimation precision, i.e., node density, sensing coverage, and heterogeneous topology.AbstractLocalization is one of the challenges in wireless sensor networks, especially those without the aid of a global positioning system. Use of a dedicated positioning device incurs additional cost and reduces battery life; therefore, a range-free localization scheme is promising as a cost-effective approach. However, the main limitation of this approach is that the estimation precision can be affected by factors such as node density, sensing coverage, and topology diversity. Thus, this study investigates and proposes a method for improving a traditional range-free-based localization method (centroid) that uses soft computing approaches in a hybrid model. This model integrates a fuzzy logic system into centroid and uses an extreme learning machine (ELM) optimization technique to capitalize on the strengths of both approaches: the former is properly used with low node density and short coverage, while the latter is used for the oppositeâ€”to achieve a robust location estimation scheme. The ratios of known nodes within the sensing coverage range to the total known nodes and of the sensing coverage range to the maximum coverage range are used as adaptive weights for this hybrid model. To further improve the efficiency, especially in heterogeneous topologies, the concept of resultant force vectors is applied to this hybrid model over particle swarm optimization to mitigate the effects of irregular deployments. The performance of the proposed method is extensively evaluated via simulations that demonstrate its effectiveness compared to other state-of-the-art soft-computing-based range-free localization schemes (i.e., centroid, a fuzzy logic system, and a support vector machine with a traditional ELM).",none
Fuzzy logic optimized wireless sensor network routing protocol,https://dl.acm.org/doi/10.5555/2595812.2595814,2,article,ACM,2013,"Wireless sensor networks WSNs are used in health monitoring, tracking and security applications. Such networks transfer data from specific areas to a nominated destination. In the network, each sensor node acts as a routing element for other sensor nodes during the transmission of data. This can increase energy consumption of the sensor node. In this paper, we propose a routing protocol for improving network lifetime and performance. The proposed protocol uses type-2 fuzzy logic to minimize the effects of uncertainty produced by the environmental noise. Simulation results show that the proposed protocol performs better than a recently developed routing protocol in terms of extending network lifetime and saving energy and also reducing data packet lost.",none
An efficient routing access method based on multi-agent reinforcement learning in UWSNs,https://dl.acm.org/doi/10.1007/s11276-021-02838-1,0,research-article,ACM,2022,"A large proportion of underwater data is collected in deep sea. Compared with the direct bottom-to-surface acoustic links, underwater sensor networks (UWSNs) with hierarchical network model topology are more efficient at transmitting huge amounts of data to sea surface. Base on reinforcement learning, an adaptive modulation and coding in depth based router (MC-DBR) algorithm was proposed. The MC-DBR is designed to reduce the energy consumption, time delay etc., while improve the communication performance. In MC-DBR, each node firstly uses HELLO packets to sense the neighbouring channel states. Then, each node updates its Q-value by multi-agent reinforcement learning based modulation and coding method (MARL-MC) algorithm. The energy consumption, the time delay, the modulation and coding methods and the packets collisions etc. are considered in MARL-MC to improve the overall performance of the whole network. The convergence and computation complexity of the MC-DBR were analyzed in detail. The performance of the MC-DBR was compared with the benchmark algorithms. The results showed that the MC-DBR can obtain lower end-to-end delay, higher packet delivery rate and lower average remaining energy of the network.",none
Reinforcement Learning Based Routing in EH-WSNs with Dual Alternative Batteries,https://dl.acm.org/doi/10.1145/3404555.3404569,0,research-article,ACM,2020,"This paper considers an Energy Harvesting Wireless Sensor Network (EH-WSN) where nodes have a dual alternative battery system. We propose a stateless distributed reinforcement learning based routing algorithm, named QLRA, where each node learns the best next hop(s) to forward its data based on the battery and data information of its neighbors. We study how the number of sources and path exploration probability impacts the performance of QLRA. Numerical results show that after learning, QLRA is able to achieve minimal end-to-end delays in all tested scenarios, which is about 18% lower than the average end-to-end delay of a competing routing algorithm.",none
